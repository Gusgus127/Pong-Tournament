{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39e6be8",
   "metadata": {},
   "source": [
    "# Part 1: Pong Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404c0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using gym version: 0.26.2\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using gym version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf8199",
   "metadata": {},
   "source": [
    "## Data Preprocessing (Wrappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45aca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    \n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1],\n",
    "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "    \n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    print(\"MaxAndSkipEnv        : {}\".format(env.observation_space.shape))\n",
    "    env = FireResetEnv(env)\n",
    "    print(\"FireResetEnv         : {}\".format(env.observation_space.shape))\n",
    "    env = ProcessFrame84(env)\n",
    "    print(\"ProcessFrame84       : {}\".format(env.observation_space.shape))\n",
    "    env = ImageToPyTorch(env)\n",
    "    print(\"ImageToPyTorch       : {}\".format(env.observation_space.shape))\n",
    "    env = BufferWrapper(env, 4)\n",
    "    print(\"BufferWrapper        : {}\".format(env.observation_space.shape))\n",
    "    env = ScaledFloatFrame(env)\n",
    "    print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "    \n",
    "    return env\n",
    "\n",
    "def print_env_info(name, env):\n",
    "    obs = env.reset()\n",
    "    print(\"*** {} Environment ***\".format(name))\n",
    "    print(\"Observation shape: {}, type: {} and range [{},{}]\".format(obs.shape, obs.dtype, np.min(obs), np.max(obs)))\n",
    "    print(\"Observation sample:\\n{}\".format(obs))\n",
    "\n",
    "#taste the environment\n",
    "ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "env = make_env(ENV_NAME)\n",
    "print_env_info(\"Wrapped\", env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c6f302",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.net(torch.zeros(1, *input_shape)).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return self.fc(x)\n",
    "    \n",
    "# Test the model\n",
    "env = make_env(ENV_NAME)\n",
    "model = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef99b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Buffer and Agent \n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer, device ):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self.device = device\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.env.reset()\n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def play_step(self, net, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy\n",
    "        Returns: reward if episode ends, else None\n",
    "        \"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(net.net[-1].out_features)\n",
    "        else:\n",
    "            state = torch.tensor(np.array([self.state]), dtype=torch.float32).to(device)\n",
    "            q_values = net(state)\n",
    "            action = int(torch.argmax(q_values, dim=1).item())\n",
    "        \n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "\n",
    "        self.exp_buffer.push(self.state, action, reward, state, done)\n",
    "\n",
    "        self.state = state\n",
    "\n",
    "        if done:\n",
    "            reward = self.total_reward\n",
    "            self.reset()\n",
    "            return reward\n",
    "\n",
    "# test the agent\n",
    "exp_buffer = ExperienceBuffer(10000)\n",
    "agent = Agent(env, exp_buffer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22602135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
    "    \"\"\"\n",
    "    Convert a batch of Experience to a tuple of tensors.\n",
    "\n",
    "    Args:\n",
    "        batch (tt.List[Experience]): A list of Experience objects.\n",
    "        device (torch.device): The device to which the tensors will be moved.\n",
    "\n",
    "    Returns:\n",
    "        BatchTensors: A tuple containing the tensors for states, actions, rewards, dones, and new_states.\n",
    "    \"\"\"\n",
    "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
    "    for e in batch:\n",
    "        states.append(e.state)\n",
    "        actions.append(e.action)\n",
    "        rewards.append(e.reward)\n",
    "        dones.append(e.done_trunc)\n",
    "        new_state.append(e.new_state)\n",
    "    states_t = torch.as_tensor(np.asarray(states))\n",
    "    actions_t = torch.LongTensor(actions)\n",
    "    rewards_t = torch.FloatTensor(rewards)\n",
    "    dones_t = torch.BoolTensor(dones)\n",
    "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
    "\n",
    "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), dones_t.to(device),  new_states_t.to(device)\n",
    "\n",
    "\n",
    "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN, device: torch.device, gamma: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the loss for a batch of experiences.\n",
    "    \n",
    "    Args:\n",
    "        batch (tt.List[Experience]): A list of Experience objects.\n",
    "        net (DQN): The current DQN network.\n",
    "        tgt_net (DQN): The target DQN network.\n",
    "        device (torch.device): The device to which the tensors will be moved.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The calculated loss.\n",
    "    \"\"\"\n",
    "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
    "\n",
    "    state_action_values = net(states_t).gather(1, actions_t.unsqueeze(-1)).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = tgt_net(new_states_t).max(1)[0]\n",
    "        next_state_values[dones_t] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * gamma + rewards_t\n",
    "    \n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "def plot_rewards(rewards, ma_window=100):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.plot(rewards, label='Rewards')\n",
    "    if len(rewards) >= ma_window:\n",
    "        ma = np.convolve(rewards, np.ones(ma_window)/ma_window, mode='valid')\n",
    "        plt.plot(range(ma_window-1, len(rewards)), ma, label='Moving Average (window={})'.format(ma_window), color='orange')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf78280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 150000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.01\n",
    "NUM_EPISODES = 5000\n",
    "\n",
    "print(f\"Training DQN on {ENV_NAME} environment\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Replay size: {REPLAY_SIZE}, Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Sync target frames: {SYNC_TARGET_FRAMES}\")\n",
    "print(f\"Epsilon start: {EPSILON_START}, Epsilon final: {EPSILON_FINAL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bc8f8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn():\n",
    "    env = make_env(ENV_NAME)\n",
    "    net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    print(net)\n",
    "\n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer)\n",
    "    epsilon = EPSILON_START\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    total_rewards = []\n",
    "    frame_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_m_reward = None\n",
    "\n",
    "    print(\">>> Training starts at \",datetime.datetime.now())\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        frame_idx += 1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "        reward = agent.play_step(net, device, epsilon)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = frame_idx\n",
    "            ts = time.time()\n",
    "            m_reward = np.mean(total_rewards[-100:])\n",
    "            print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
    "            wandb.log({\"epsilon\": epsilon, \"speed\": speed, \"reward_100\": m_reward, \"reward\": reward}, step=frame_idx)\n",
    "\n",
    "            if best_m_reward is None or best_m_reward < m_reward:\n",
    "                if best_m_reward is not None:\n",
    "                    print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
    "                    model_name = os.path.join(\"models\", ENV_NAME + \"_DQN.pth\")\n",
    "                    print(f\"Saving model '{model_name}'\")\n",
    "                    torch.save(net.state_dict(), model_name)\n",
    "                best_m_reward = m_reward\n",
    "            if m_reward > MEAN_REWARD_BOUND:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        loss_t = calc_loss(batch, net, tgt_net, device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plotting rewards\n",
    "        if episode % 50 == 0 and episode > 0:\n",
    "            plot_rewards(total_rewards)\n",
    "    \n",
    "    print(\">>> Training ends at \",datetime.datetime.now())\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "# Start training\n",
    "dqn_rewards = train_dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4fa79",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4207d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
