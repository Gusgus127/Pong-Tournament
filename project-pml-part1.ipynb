{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b39e6be8","cell_type":"markdown","source":"# Part 1: Pong Tournament","metadata":{}},{"id":"c7f77504-51d7-4faf-ab7a-46e47e1f4123","cell_type":"code","source":"!pip install gymnasium[atari]\n!pip install gymnasium[accept-rom-license]\n!pip install ale-py\n!pip install autorom\n!AutoROM --accept-license","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:41:16.099442Z","iopub.execute_input":"2025-11-27T14:41:16.099705Z","iopub.status.idle":"2025-11-27T14:41:31.012093Z","shell.execute_reply.started":"2025-11-27T14:41:16.099687Z","shell.execute_reply":"2025-11-27T14:41:31.011348Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (0.29.0)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\nRequirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (2.4.1)\nRequirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.11/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.5.2)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[atari]) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[atari]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[atari]) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium[atari]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium[atari]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium[atari]) (2024.2.0)\nRequirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.29.0)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\nRequirement already satisfied: autorom~=0.4.2 in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.4.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.67.1)\nRequirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.6.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium[accept-rom-license]) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2025.10.5)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (2024.2.0)\nRequirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.8.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ale-py) (1.26.4)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py) (6.5.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ale-py) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ale-py) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ale-py) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->ale-py) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->ale-py) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->ale-py) (2024.2.0)\nRequirement already satisfied: autorom in /usr/local/lib/python3.11/dist-packages (0.4.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom) (8.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom) (4.67.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2025.10.5)\nAutoROM will download the Atari 2600 ROMs.\nThey will be installed to:\n\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n\nExisting ROMs will be overwritten.\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/adventure.bin    \nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/air_raid.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/alien.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/amidar.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/assault.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asterix.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asteroids.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis2.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/backgammon.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bank_heist.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/basic_math.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/battle_zone.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/beam_rider.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/berzerk.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/blackjack.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bowling.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/boxing.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/breakout.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/carnival.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/casino.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/centipede.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/chopper_command.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/combat.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crazy_climber.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crossbow.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/darkchambers.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/defender.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/demon_attack.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/donkey_kong.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/double_dunk.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/earthworld.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/elevator_action.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/enduro.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/entombed.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/et.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/fishing_derby.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/flag_capture.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/freeway.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frogger.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frostbite.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/galaxian.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gopher.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gravitar.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hangman.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/haunted_house.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hero.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/human_cannonball.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ice_hockey.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/jamesbond.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/journey_escape.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/joust.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kaboom.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kangaroo.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/keystone_kapers.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/king_kong.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/klax.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/koolaid.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/krull.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kung_fu_master.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/laser_gates.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/lost_luggage.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mario_bros.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/maze_craze.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/miniature_golf.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/montezuma_revenge.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mr_do.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ms_pacman.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/name_this_game.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/othello.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pacman.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/phoenix.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall2.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pong.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pooyan.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/private_eye.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/qbert.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/riverraid.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/road_runner.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/robotank.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/seaquest.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/sir_lancelot.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/skiing.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/solaris.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_invaders.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_war.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/star_gunner.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/superman.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/surround.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tennis.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tetris.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/time_pilot.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/trondead.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/turmoil.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tutankham.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/up_n_down.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/venture.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_checkers.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_chess.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_cube.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_pinball.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/warlords.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/wizard_of_wor.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/word_zapper.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/yars_revenge.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/zaxxon.bin\nDone!\n","output_type":"stream"}],"execution_count":1},{"id":"9744af11-c9a1-4eb4-aaa8-bfa183ee01bf","cell_type":"code","source":"import gymnasium as gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport collections\nimport random\nimport time\nimport datetime\nfrom collections import deque, namedtuple\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\nprint(f\"Using Gymnasium version: {gym.__version__}\")\n\n# Define the Experience tuple\nExperience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'new_state', 'done'])\n\n# Fixed Environment Wrappers for Gymnasium\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        # Check if fire action is available\n        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, terminated, truncated, _ = self.env.step(1)\n        if terminated or truncated:\n            self.env.reset(**kwargs)\n        obs, _, terminated, truncated, _ = self.env.step(2)\n        if terminated or truncated:\n            self.env.reset(**kwargs)\n        return obs, {}\n\n    def step(self, action):\n        return self.env.step(action)\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        super().__init__(env)\n        self._obs_buffer = collections.deque(maxlen=2)\n        self._skip = skip\n\n    def step(self, action):\n        total_reward = 0.0\n        terminated = False\n        truncated = False\n        info = {}\n        \n        for _ in range(self._skip):\n            obs, reward, terminated, truncated, info = self.env.step(action)\n            self._obs_buffer.append(obs)\n            total_reward += reward\n            if terminated or truncated:\n                break\n        \n        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n        return max_frame, total_reward, terminated, truncated, info\n\n    def reset(self, **kwargs):\n        self._obs_buffer.clear()\n        obs, info = self.env.reset(**kwargs)\n        self._obs_buffer.append(obs)\n        return obs, info\n\nclass ProcessFrame84(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n\n    def observation(self, obs):\n        return self._process_frame(obs)\n\n    def _process_frame(self, frame):\n        if frame.size == 210 * 160 * 3:\n            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n        elif frame.size == 250 * 160 * 3:\n            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n        else:\n            # Try to handle different frame sizes\n            img = frame.astype(np.float32)\n            if len(img.shape) == 3 and img.shape[2] == 3:\n                pass  # Already in correct format\n            else:\n                img = img.reshape((210, 160, 3))\n        \n        # Convert to grayscale\n        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n        \n        # Resize\n        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n        x_t = resized_screen[18:102, :]\n        x_t = np.reshape(x_t, [84, 84, 1])\n        return x_t.astype(np.uint8)\n\nclass BufferWrapper(gym.ObservationWrapper):\n    def __init__(self, env, n_steps):\n        super().__init__(env)\n        self.n_steps = n_steps\n        old_space = env.observation_space\n        new_shape = (n_steps * old_space.shape[0], old_space.shape[1], old_space.shape[2])\n        \n        self.observation_space = gym.spaces.Box(\n            low=0.0, high=1.0, shape=new_shape, dtype=old_space.dtype\n        )\n        self.buffer = deque(maxlen=n_steps)\n\n    def reset(self, **kwargs):\n        self.buffer.clear()\n        obs, info = self.env.reset(**kwargs)\n        for _ in range(self.n_steps):\n            self.buffer.append(obs)\n        return self._get_obs(), info\n\n    def observation(self, observation):\n        self.buffer.append(observation)\n        return self._get_obs()\n\n    def _get_obs(self):\n        return np.concatenate(self.buffer, axis=0)\n\nclass ImageToPyTorch(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        old_shape = self.observation_space.shape\n        # Change from HWC to CHW\n        new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n        self.observation_space = gym.spaces.Box(\n            low=0.0, high=1.0, shape=new_shape, dtype=np.float32\n        )\n\n    def observation(self, observation):\n        # Move channels to first dimension\n        return np.moveaxis(observation, -1, 0)\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def observation(self, obs):\n        return np.array(obs, dtype=np.float32) / 255.0\n\ndef make_env(env_name):\n    env = gym.make(env_name, render_mode='rgb_array')\n    env = MaxAndSkipEnv(env)\n    env = FireResetEnv(env)\n    env = ProcessFrame84(env)\n    env = ImageToPyTorch(env)\n    env = ScaledFloatFrame(env)\n    env = BufferWrapper(env, 4)\n    return env\n\n# DQN Network\nclass DQN(nn.Module):\n    def __init__(self, input_shape, n_actions):\n        super(DQN, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU(),\n        )\n\n        # Calculate the size of the flattened features\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, *input_shape)\n            conv_out = self.conv(dummy_input)\n            n_flatten = conv_out.view(1, -1).size(1)\n\n        self.fc = nn.Sequential(\n            nn.Linear(n_flatten, 512),\n            nn.ReLU(),\n            nn.Linear(512, n_actions)\n        )\n\n    def forward(self, x):\n        # x shape: [batch_size, channels, height, width]\n        conv_out = self.conv(x)\n        flattened = conv_out.view(conv_out.size(0), -1)\n        return self.fc(flattened)\n\n# Experience Buffer\nclass ExperienceBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def __len__(self):\n        return len(self.buffer)\n\n    def push(self, state, action, reward, next_state, done):\n        experience = Experience(state, action, reward, next_state, done)\n        self.buffer.append(experience)\n\n    def sample(self, batch_size):\n        if len(self.buffer) < batch_size:\n            batch_size = len(self.buffer)\n            \n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        \n        return (\n            np.array(states, dtype=np.float32),\n            np.array(actions, dtype=np.int64),\n            np.array(rewards, dtype=np.float32),\n            np.array(next_states, dtype=np.float32),\n            np.array(dones, dtype=np.bool_)\n        )\n\n# Agent\nclass Agent:\n    def __init__(self, env, exp_buffer):\n        self.env = env\n        self.exp_buffer = exp_buffer\n        self.reset()\n\n    def reset(self):\n        self.state, _ = self.env.reset()\n        self.total_reward = 0.0\n\n    @torch.no_grad()\n    def play_step(self, net, epsilon=0.0, device=device):\n        done_reward = None\n\n        if np.random.random() < epsilon:\n            action = self.env.action_space.sample()\n        else:\n            state_a = np.array([self.state], copy=False)\n            state_v = torch.tensor(state_a, dtype=torch.float32).to(device)\n            q_vals_v = net(state_v)\n            _, act_v = torch.max(q_vals_v, dim=1)\n            action = int(act_v.item())\n\n        # Take step in environment\n        new_state, reward, terminated, truncated, _ = self.env.step(action)\n        done = terminated or truncated\n\n        reward = np.clip(reward, -1.0, 1.0)\n        # Store the ACTUAL step reward, not cumulative\n        self.exp_buffer.push(self.state, action, reward, new_state, done)\n        \n        self.state = new_state\n        self.total_reward += reward\n\n        if done:\n            done_reward = self.total_reward\n            self.reset()\n            \n        return done_reward\n\n# Loss calculation functions\ndef calc_loss(batch, net, tgt_net, device, gamma=0.99):\n    states, actions, rewards, next_states, dones = batch\n    \n    # Convert to tensors\n    states_v = torch.tensor(states).to(device)\n    actions_v = torch.tensor(actions).to(device)\n    rewards_v = torch.tensor(rewards).to(device)\n    next_states_v = torch.tensor(next_states).to(device)\n    dones_mask = torch.tensor(dones, dtype=torch.bool).to(device)\n    \n    # Current Q values\n    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n    \n    # Next Q values from target network\n    with torch.no_grad():\n        next_state_values = tgt_net(next_states_v).max(1)[0]\n        next_state_values[dones_mask] = 0.0\n        expected_state_action_values = next_state_values * gamma + rewards_v\n    \n    # Compute loss\n    loss = nn.MSELoss()(state_action_values, expected_state_action_values)\n    return loss\n\n# Plotting function\ndef plot_rewards(rewards, ma_window=100):\n    plt.figure(figsize=(12, 8))\n    plt.title(\"Training Rewards\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Reward\")\n    \n    if len(rewards) > 0:\n        plt.plot(rewards, alpha=0.3, label='Raw rewards', color='blue')\n        \n        if len(rewards) >= ma_window:\n            ma_rewards = []\n            for i in range(len(rewards) - ma_window + 1):\n                ma_rewards.append(np.mean(rewards[i:i+ma_window]))\n            plt.plot(range(ma_window-1, len(rewards)), ma_rewards, \n                    label=f'Moving Average ({ma_window} episodes)', color='red', linewidth=2)\n        \n        plt.legend()\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()\n\n# Training function\ndef train_dqn():\n    # Create models directory\n    os.makedirs(\"models\", exist_ok=True)\n    \n    # Parameters\n    ENV_NAME = \"PongNoFrameskip-v4\"\n    MEAN_REWARD_BOUND = 18.0  # Pong is solved when average reward > 18\n    \n    GAMMA = 0.99\n    BATCH_SIZE = 32\n    REPLAY_SIZE = 50000\n    LEARNING_RATE = 1e-4\n    SYNC_TARGET_FRAMES = 1000\n    REPLAY_START_SIZE = 10000\n    \n    EPSILON_DECAY_LAST_FRAME = 50000\n    EPSILON_START = 1.0\n    EPSILON_FINAL = 0.02\n    \n    print(f\"Training DQN on {ENV_NAME}\")\n    print(f\"Device: {device}\")\n    \n    # Create environment\n    env = make_env(ENV_NAME)\n    \n    # Initialize networks\n    net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n    tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n    tgt_net.load_state_dict(net.state_dict())\n    \n    print(f\"Network: {net}\")\n    print(f\"Input shape: {env.observation_space.shape}\")\n    print(f\"Number of actions: {env.action_space.n}\")\n    \n    # Initialize buffer and agent\n    buffer = ExperienceBuffer(REPLAY_SIZE)\n    agent = Agent(env, buffer)\n    \n    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n    total_rewards = []\n    frame_idx = 0\n    ts_frame = 0\n    ts = time.time()\n    best_mean_reward = None\n    \n    print(\"Starting training at:\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n    print(\"Filling replay buffer...\")\n    \n    # Initial filling of replay buffer\n    while len(buffer) < REPLAY_START_SIZE:\n        frame_idx += 1\n        epsilon = 1.0  # Always explore during initial filling\n        reward = agent.play_step(net, epsilon, device)\n        if reward is not None:\n            total_rewards.append(reward)\n            \n        if frame_idx % 1000 == 0:\n            print(f\"Filled {len(buffer)}/{REPLAY_START_SIZE} experiences in replay buffer\")\n    \n    print(\"Replay buffer filled. Starting training...\")\n    \n    for episode in range(10000):\n        frame_idx += 1\n        epsilon = EPSILON_FINAL + (EPSILON_START - EPSILON_FINAL) * max(0, 1 - frame_idx / EPSILON_DECAY_LAST_FRAME)\n        \n        reward = agent.play_step(net, epsilon, device)\n        if reward is not None:\n            total_rewards.append(reward)\n            mean_reward = np.mean(total_rewards[-100:]) if len(total_rewards) >= 100 else np.mean(total_rewards)\n            \n            # Calculate speed\n            speed = (frame_idx - ts_frame) / (time.time() - ts)\n            ts_frame = frame_idx\n            ts = time.time()\n            \n            print(f\"Frame {frame_idx}: Episode {len(total_rewards)}, Reward: {reward:.1f}, \"\n                  f\"Mean Reward (last 100): {mean_reward:.3f}, Epsilon: {epsilon:.3f}, Speed: {speed:.2f} f/s\")\n            \n            # Save best model\n            if best_mean_reward is None or mean_reward > best_mean_reward:\n                if best_mean_reward is not None:\n                    print(f\"New best mean reward: {best_mean_reward:.3f} -> {mean_reward:.3f}\")\n                best_mean_reward = mean_reward\n                model_path = f\"models/{ENV_NAME}_best.pth\"\n                torch.save(net.state_dict(), model_path)\n                print(f\"Model saved to {model_path}\")\n            \n            # Check if solved\n            if mean_reward >= MEAN_REWARD_BOUND:\n                print(f\"Solved at frame {frame_idx} with mean reward {mean_reward:.3f}!\")\n                break\n        \n        # Sync target network\n        if frame_idx % SYNC_TARGET_FRAMES == 0:\n            tgt_net.load_state_dict(net.state_dict())\n            print(f\"Target network updated at frame {frame_idx}\")\n        \n        # Training step\n        optimizer.zero_grad()\n        batch = buffer.sample(BATCH_SIZE)\n        loss_t = calc_loss(batch, net, tgt_net, device, GAMMA)\n        loss_t.backward()\n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n        optimizer.step()\n        \n        # Plot progress every 50 episodes\n        if len(total_rewards) % 50 == 0 and len(total_rewards) > 0:\n            plot_rewards(total_rewards)\n    \n    print(\"Training completed at:\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n    return total_rewards, net","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:41:31.013595Z","iopub.execute_input":"2025-11-27T14:41:31.013833Z","iopub.status.idle":"2025-11-27T14:41:35.606783Z","shell.execute_reply.started":"2025-11-27T14:41:31.013809Z","shell.execute_reply":"2025-11-27T14:41:35.605744Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nUsing Gymnasium version: 0.29.0\n","output_type":"stream"}],"execution_count":2},{"id":"7404c0bc","cell_type":"code","source":"import torch.nn.functional as F\n\nclass PolicyNetwork(nn.Module):\n    \"\"\"REINFORCE Policy Network\"\"\"\n    def __init__(self, input_shape, n_actions):\n        super(PolicyNetwork, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n        \n        with torch.no_grad():\n            n_flatten = self.conv(torch.zeros(1, *input_shape)).shape[1]\n            \n        self.fc = nn.Sequential(\n            nn.Linear(n_flatten, 512),\n            nn.ReLU(),\n            nn.Linear(512, n_actions)\n        )\n    \n    def forward(self, x):\n        x = self.conv(x)\n        return F.softmax(self.fc(x), dim=-1)\n\nclass REINFORCEAgent:\n    def __init__(self, env):\n        self.env = env\n        self.net = PolicyNetwork(env.observation_space.shape, env.action_space.n).to(device)\n        self.optimizer = optim.Adam(self.net.parameters(), lr=1e-4)\n        self.saved_log_probs = []\n        self.rewards = []\n    \n    def act(self, state):\n        state_v = torch.tensor(np.array([state]), dtype=torch.float32).to(device)\n        probs = self.net(state_v)\n        m = torch.distributions.Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs.append(m.log_prob(action))\n        return action.item()\n    \n    def update_policy(self, gamma=0.99):\n        returns = []\n        R = 0\n        for r in self.rewards[::-1]:\n            R = r + gamma * R\n            returns.insert(0, R)\n        \n        returns = torch.tensor(returns).to(device)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n        \n        policy_loss = []\n        for log_prob, R in zip(self.saved_log_probs, returns):\n            policy_loss.append(-log_prob * R)\n        \n        self.optimizer.zero_grad()\n        policy_loss = torch.cat(policy_loss).sum()\n        policy_loss.backward()\n        self.optimizer.step()\n        \n        self.saved_log_probs = []\n        self.rewards = []\n    \n    def train(self, episodes=1000):\n        total_rewards = []\n        \n        for episode in range(episodes):\n            state, _ = self.env.reset()\n            episode_reward = 0\n            done = False\n            \n            while not done:\n                action = self.act(state)\n                state, reward, terminated, truncated, _ = self.env.step(action)\n                done = terminated or truncated\n                self.rewards.append(reward)\n                episode_reward += reward\n            \n            self.update_policy()\n            total_rewards.append(episode_reward)\n            \n            if episode % 50 == 0:\n                avg_reward = np.mean(total_rewards[-50:]) if len(total_rewards) >= 50 else np.mean(total_rewards)\n                print(f\"REINFORCE Episode {episode}, Reward: {episode_reward}, Avg: {avg_reward:.2f}\")\n        \n        return total_rewards","metadata":{"execution":{"iopub.status.busy":"2025-11-27T14:46:06.385311Z","iopub.execute_input":"2025-11-27T14:46:06.386141Z","iopub.status.idle":"2025-11-27T14:46:06.398123Z","shell.execute_reply.started":"2025-11-27T14:46:06.386114Z","shell.execute_reply":"2025-11-27T14:46:06.397406Z"},"trusted":true},"outputs":[],"execution_count":6},{"id":"1866b087-c18b-4910-9672-3fa45d5b1c20","cell_type":"code","source":"def compare_models(dqn_rewards, reinforce_rewards):\n    \"\"\"Compare both models and determine the best one\"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    # Smooth rewards for better visualization\n    window = 100\n    if len(dqn_rewards) >= window:\n        dqn_smooth = np.convolve(dqn_rewards, np.ones(window)/window, mode='valid')\n        reinforce_smooth = np.convolve(reinforce_rewards, np.ones(window)/window, mode='valid')\n        \n        plt.plot(range(window-1, len(dqn_rewards)), dqn_smooth, label='DQN', linewidth=2)\n        plt.plot(range(window-1, len(reinforce_rewards)), reinforce_smooth, label='REINFORCE', linewidth=2)\n    else:\n        plt.plot(dqn_rewards, label='DQN', alpha=0.7)\n        plt.plot(reinforce_rewards, label='REINFORCE', alpha=0.7)\n    \n    plt.xlabel('Episode')\n    plt.ylabel('Smoothed Reward')\n    plt.title('DQN vs REINFORCE Performance Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # Final analysis\n    dqn_final_avg = np.mean(dqn_rewards[-100:]) if len(dqn_rewards) >= 100 else np.mean(dqn_rewards)\n    reinforce_final_avg = np.mean(reinforce_rewards[-100:]) if len(reinforce_rewards) >= 100 else np.mean(reinforce_rewards)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL MODEL COMPARISON\")\n    print(\"=\"*60)\n    print(f\"DQN Final Average Reward (last 100 episodes): {dqn_final_avg:.3f}\")\n    print(f\"REINFORCE Final Average Reward (last 100 episodes): {reinforce_final_avg:.3f}\")\n    \n    return \"DQN\" if dqn_final_avg > reinforce_final_avg else \"REINFORCE\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:46:12.456597Z","iopub.execute_input":"2025-11-27T14:46:12.457342Z","iopub.status.idle":"2025-11-27T14:46:12.464256Z","shell.execute_reply.started":"2025-11-27T14:46:12.457312Z","shell.execute_reply":"2025-11-27T14:46:12.463531Z"}},"outputs":[],"execution_count":7},{"id":"c2c87b0b-37e7-4f8e-b5d5-9aefb35f6b7b","cell_type":"code","source":"if __name__ == \"__main__\":\n    print(\"Starting Part 1: Solving Pong with DQN and REINFORCE\")\n    \n    # Train DQN\n    print(\"\\n\" + \"=\"*50)\n    print(\"TRAINING DQN\")\n    print(\"=\"*50)\n    dqn_rewards, trained_dqn = train_dqn()\n    \n    # Train REINFORCE\n    print(\"\\n\" + \"=\"*50)\n    print(\"TRAINING REINFORCE\")\n    print(\"=\"*50)\n    env = make_env(\"PongNoFrameskip-v4\")\n    reinforce_agent = REINFORCEAgent(env)\n    reinforce_rewards = reinforce_agent.train(episodes=1000)\n    \n    # Compare models\n    print(\"\\n\" + \"=\"*50)\n    print(\"MODEL COMPARISON\")\n    print(\"=\"*50)\n    best_model = compare_models(dqn_rewards, reinforce_rewards)\n    \n    print(f\"\\nðŸ“‹ Part 1 Summary:\")\n    print(f\"- Environment: PongNoFrameskip-v4\")\n    print(f\"- Models implemented: DQN and REINFORCE\")\n    print(f\"- Best performing model: {best_model}\")\n    print(f\"- Final DQN performance: {np.mean(dqn_rewards[-100:]):.3f} (last 100 episodes)\")\n    print(f\"- Final REINFORCE performance: {np.mean(reinforce_rewards[-100:]):.3f} (last 100 episodes)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:41:35.644326Z","iopub.execute_input":"2025-11-27T14:41:35.644514Z","iopub.status.idle":"2025-11-27T14:43:20.428501Z","shell.execute_reply.started":"2025-11-27T14:41:35.644499Z","shell.execute_reply":"2025-11-27T14:43:20.427293Z"}},"outputs":[{"name":"stdout","text":"Starting Part 1: Solving Pong with DQN and REINFORCE\n\n==================================================\nTRAINING DQN\n==================================================\nTraining DQN on PongNoFrameskip-v4\nDevice: cuda\n","output_type":"stream"},{"name":"stderr","text":"A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n[Powered by Stella]\n","output_type":"stream"},{"name":"stdout","text":"Network: DQN(\n  (conv): Sequential(\n    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU()\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=3136, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=6, bias=True)\n  )\n)\nInput shape: (4, 84, 84)\nNumber of actions: 6\nStarting training at: 2025-11-27 14:41:38\nFilling replay buffer...\nFilled 1000/10000 experiences in replay buffer\nFilled 2000/10000 experiences in replay buffer\nFilled 3000/10000 experiences in replay buffer\nFilled 4000/10000 experiences in replay buffer\nFilled 5000/10000 experiences in replay buffer\nFilled 6000/10000 experiences in replay buffer\nFilled 7000/10000 experiences in replay buffer\nFilled 8000/10000 experiences in replay buffer\nFilled 9000/10000 experiences in replay buffer\nFilled 10000/10000 experiences in replay buffer\nReplay buffer filled. Starting training...\nFrame 10056: Episode 11, Reward: -21.0, Mean Reward (last 100): -20.818, Epsilon: 0.803, Speed: 670.14 f/s\nModel saved to models/PongNoFrameskip-v4_best.pth\nFrame 10926: Episode 12, Reward: -21.0, Mean Reward (last 100): -20.833, Epsilon: 0.786, Speed: 110.46 f/s\nTarget network updated at frame 11000\nFrame 11808: Episode 13, Reward: -21.0, Mean Reward (last 100): -20.846, Epsilon: 0.769, Speed: 117.31 f/s\nTarget network updated at frame 12000\nFrame 12630: Episode 14, Reward: -21.0, Mean Reward (last 100): -20.857, Epsilon: 0.752, Speed: 116.11 f/s\nTarget network updated at frame 13000\nFrame 13452: Episode 15, Reward: -21.0, Mean Reward (last 100): -20.867, Epsilon: 0.736, Speed: 117.19 f/s\nTarget network updated at frame 14000\nFrame 14262: Episode 16, Reward: -21.0, Mean Reward (last 100): -20.875, Epsilon: 0.720, Speed: 116.95 f/s\nTarget network updated at frame 15000\nFrame 15024: Episode 17, Reward: -21.0, Mean Reward (last 100): -20.882, Epsilon: 0.706, Speed: 112.36 f/s\nFrame 15786: Episode 18, Reward: -21.0, Mean Reward (last 100): -20.889, Epsilon: 0.691, Speed: 108.01 f/s\nTarget network updated at frame 16000\nFrame 16636: Episode 19, Reward: -21.0, Mean Reward (last 100): -20.895, Epsilon: 0.674, Speed: 109.50 f/s\nTarget network updated at frame 17000\nFrame 17486: Episode 20, Reward: -21.0, Mean Reward (last 100): -20.900, Epsilon: 0.657, Speed: 115.41 f/s\nTarget network updated at frame 18000\nFrame 18417: Episode 21, Reward: -19.0, Mean Reward (last 100): -20.810, Epsilon: 0.639, Speed: 117.46 f/s\nNew best mean reward: -20.818 -> -20.810\nModel saved to models/PongNoFrameskip-v4_best.pth\nTarget network updated at frame 19000\nFrame 19179: Episode 22, Reward: -21.0, Mean Reward (last 100): -20.818, Epsilon: 0.624, Speed: 115.65 f/s\nTarget network updated at frame 20000\nTraining completed at: 2025-11-27 14:43:20\n\n==================================================\nTRAINING REINFORCE\n==================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_137/1815163091.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PongNoFrameskip-v4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mreinforce_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREINFORCEAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mreinforce_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinforce_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Compare models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_137/3832427317.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_137/3832427317.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mstate_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_137/3832427317.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mREINFORCEAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"],"ename":"NameError","evalue":"name 'F' is not defined","output_type":"error"}],"execution_count":5},{"id":"46bf8199","cell_type":"markdown","source":"## Data Preprocessing (Wrappers)","metadata":{}},{"id":"d45aca8c","cell_type":"code","source":"class FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, term, trunc, _ = self.env.step(1)\n        if term or trunc:\n            self.env.reset(**kwargs)\n        obs, _, term, trunc, _ = self.env.step(2)\n        if term or trunc:\n            self.env.reset(**kwargs)\n        return obs, {}\n\n    def step(self, action):\n        return self.env.step(action)\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        super().__init__(env)\n        self.buffer = collections.deque(maxlen=2)\n        self.skip = skip\n\n    def step(self, action):\n        total_reward = 0\n        term = False\n        trunc = False\n        info = {}\n        for _ in range(self.skip):\n            obs, reward, term, trunc, info = self.env.step(action)\n            self.buffer.append(obs)\n            total_reward += reward\n            if term or trunc:\n                break\n        max_frame = np.max(np.stack(self.buffer), axis=0)\n        return max_frame, total_reward, term, trunc, info\n\n    def reset(self, **kwargs):\n        self.buffer.clear()\n        obs, info = self.env.reset(**kwargs)\n        self.buffer.append(obs)\n        return obs, info\n\n\nclass BufferWrapper(gym.ObservationWrapper):\n    def __init__(self, env, n_steps):\n        super().__init__(env)\n        self.n_steps = n_steps\n        old = env.observation_space\n        self.observation_space = gym.spaces.Box(\n            low=old.low.repeat(n_steps,0),\n            high=old.high.repeat(n_steps,0),\n            dtype=old.dtype\n        )\n        self.buffer = deque(maxlen=n_steps)\n\n    def reset(self, **kwargs):\n        self.buffer.clear()\n        obs, info = self.env.reset(**kwargs)\n        for _ in range(self.n_steps):\n            self.buffer.append(obs)\n        return self._get(), info\n\n    def observation(self, obs):\n        self.buffer.append(obs)\n        return self._get()\n\n    def _get(self):\n        return np.stack(self.buffer, axis=0)\n\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def observation(self, obs):\n        return np.array(obs, dtype=np.float32) / 255.0\n\ndef make_env(env_name):\n    env = gym.make(env_name, render_mode='rgb_array')\n    env = MaxAndSkipEnv(env)\n    env = FireResetEnv(env)\n    env = ProcessFrame84(env)\n    env = BufferWrapper(env, 4)\n    env = ScaledFloatFrame(env)\n    return env","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:43:20.429147Z","iopub.status.idle":"2025-11-27T14:43:20.429409Z","shell.execute_reply.started":"2025-11-27T14:43:20.429279Z","shell.execute_reply":"2025-11-27T14:43:20.429290Z"}},"outputs":[],"execution_count":null},{"id":"97c6f302","cell_type":"markdown","source":"## DQN","metadata":{}},{"id":"facd02b9","cell_type":"code","source":"class DQN(nn.Module):\n    def __init__(self, input_shape, n_actions):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_shape[0], 32, 8, stride=4), nn.ReLU(),\n            nn.Conv2d(32, 64, 4, stride=2), nn.ReLU(),\n            nn.Conv2d(64, 64, 3, stride=1), nn.ReLU(),\n            nn.Flatten()\n        )\n        with torch.no_grad():\n            n_flatten = self.conv(torch.zeros(1, *input_shape)).shape[1]\n\n        self.fc = nn.Sequential(\n            nn.Linear(n_flatten, 512), nn.ReLU(),\n            nn.Linear(512, n_actions)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:43:20.430560Z","iopub.status.idle":"2025-11-27T14:43:20.430827Z","shell.execute_reply.started":"2025-11-27T14:43:20.430692Z","shell.execute_reply":"2025-11-27T14:43:20.430708Z"}},"outputs":[],"execution_count":null},{"id":"5ef99b2f","cell_type":"code","source":"class ExperienceBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def __len__(self):\n        return len(self.buffer)\n\n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append(Experience(state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        batch_size = min(batch_size, len(self.buffer))\n        batch = random.sample(self.buffer, batch_size)\n        s, a, r, ns, d = zip(*batch)\n        return (\n            np.array(s, dtype=np.float32),\n            np.array(a, dtype=np.int64),\n            np.array(r, dtype=np.float32),\n            np.array(ns, dtype=np.float32),\n            np.array(d, dtype=np.bool_)\n        )\n\n\nclass Agent:\n    def __init__(self, env, buffer):\n        self.env = env\n        self.buffer = buffer\n        self.reset()\n\n    def reset(self):\n        self.state, _ = self.env.reset()\n        self.total_reward = 0\n\n    @torch.no_grad()\n    def play_step(self, net, epsilon=0.0, device=device):\n        if np.random.random() < epsilon:\n            action = self.env.action_space.sample()\n        else:\n            s = torch.tensor([self.state], dtype=torch.float32).to(device)\n            q = net(s)\n            action = int(torch.argmax(q, dim=1).item())\n\n        new_state, reward, term, trunc, _ = self.env.step(action)\n        done = term or trunc\n        self.total_reward += reward\n\n        self.buffer.push(self.state, action, reward, new_state, done)\n        self.state = new_state\n\n        if done:\n            r = self.total_reward\n            self.reset()\n            return r\n        return None\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:43:20.431685Z","iopub.status.idle":"2025-11-27T14:43:20.432065Z","shell.execute_reply.started":"2025-11-27T14:43:20.431862Z","shell.execute_reply":"2025-11-27T14:43:20.431896Z"}},"outputs":[],"execution_count":null},{"id":"22602135","cell_type":"code","source":"def calc_loss(batch, net, tgt_net, device, gamma=0.99):\n    states, actions, rewards, next_states, dones = batch\n\n    s = torch.tensor(states).to(device)\n    a = torch.tensor(actions).to(device)\n    r = torch.tensor(rewards).to(device)\n    ns = torch.tensor(next_states).to(device)\n    d = torch.tensor(dones, dtype=torch.bool).to(device)\n\n    q_vals = net(s).gather(1, a.unsqueeze(-1)).squeeze(-1)\n\n    with torch.no_grad():\n        next_q = tgt_net(ns).max(1)[0]\n        next_q[d] = 0.0\n        expected = r + gamma * next_q\n\n    return nn.MSELoss()(q_vals, expected)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:43:20.433242Z","iopub.status.idle":"2025-11-27T14:43:20.433491Z","shell.execute_reply.started":"2025-11-27T14:43:20.433375Z","shell.execute_reply":"2025-11-27T14:43:20.433385Z"}},"outputs":[],"execution_count":null},{"id":"ebf78280","cell_type":"code","source":"def plot_rewards(rewards, ma_window=100):\n    plt.figure(figsize=(12, 8))\n    plt.title(\"Training Rewards\")\n    plt.plot(rewards, alpha=0.3)\n    \n    if len(rewards) >= ma_window:\n        ma = [np.mean(rewards[i:i+ma_window]) \n              for i in range(len(rewards)-ma_window+1)]\n        plt.plot(range(ma_window-1, len(rewards)), ma, linewidth=2)\n\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Reward\")\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:43:20.434693Z","iopub.status.idle":"2025-11-27T14:43:20.435010Z","shell.execute_reply.started":"2025-11-27T14:43:20.434846Z","shell.execute_reply":"2025-11-27T14:43:20.434861Z"}},"outputs":[],"execution_count":null},{"id":"885bc8f8","cell_type":"markdown","source":"## Training","metadata":{}},{"id":"c8a2fdbc","cell_type":"code","source":"def train_dqn():\n    os.makedirs(\"models\", exist_ok=True)\n\n    ENV_NAME = \"PongNoFrameskip-v4\"\n    MEAN_REWARD_BOUND = 18.0\n\n    GAMMA = 0.99\n    BATCH_SIZE = 32\n    REPLAY_SIZE = 10000\n    LR = 1e-4\n    SYNC_FRAMES = 1000\n    REPLAY_START = 10000\n\n    EPS_LAST = 100000\n    EPS_START = 1.0\n    EPS_END = 0.02\n\n    env = make_env(ENV_NAME)\n    net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n    tgt = DQN(env.observation_space.shape, env.action_space.n).to(device)\n    tgt.load_state_dict(net.state_dict())\n\n    buffer = ExperienceBuffer(REPLAY_SIZE)\n    agent = Agent(env, buffer)\n\n    opt = optim.Adam(net.parameters(), lr=LR)\n    rewards = []\n    frame_idx = 0\n    best_mean = None\n\n    while len(buffer) < REPLAY_START:\n        agent.play_step(net, epsilon=1.0)\n        frame_idx += 1\n\n    for episode in range(10000):\n        frame_idx += 1\n        epsilon = max(EPS_END, EPS_START - frame_idx / EPS_LAST)\n\n        reward = agent.play_step(net, epsilon)\n        if reward is not None:\n            rewards.append(reward)\n            mean = np.mean(rewards[-100:])\n            print(f\"Frame {frame_idx}, Episode {len(rewards)}, Reward {reward:.1f}, Mean {mean:.2f}, Îµ={epsilon:.3f}\")\n\n            if best_mean is None or mean > best_mean:\n                best_mean = mean\n                torch.save(net.state_dict(), f\"models/{ENV_NAME}_best.pth\")\n\n            if mean >= MEAN_REWARD_BOUND:\n                print(\"Solved!\")\n                break\n\n        if frame_idx % SYNC_FRAMES == 0:\n            tgt.load_state_dict(net.state_dict())\n\n        opt.zero_grad()\n        batch = buffer.sample(BATCH_SIZE)\n        loss = calc_loss(batch, net, tgt, device, GAMMA)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n        opt.step()\n\n    return rewards, net","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:43:20.435934Z","iopub.status.idle":"2025-11-27T14:43:20.436239Z","shell.execute_reply.started":"2025-11-27T14:43:20.436065Z","shell.execute_reply":"2025-11-27T14:43:20.436075Z"}},"outputs":[],"execution_count":null},{"id":"1c89fc59-620f-4c6b-a1ac-5512d4d37aa7","cell_type":"code","source":"rewards, trained_net = train_dqn()\n\nplot_rewards(rewards)\n\nif len(rewards) >= 100:\n    best100 = max(np.mean(rewards[i:i+100]) for i in range(len(rewards)-99))\n    print(f\"Best 100-episode mean reward: {best100:.3f}\")\nelse:\n    print(f\"Mean reward: {np.mean(rewards):.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T14:43:20.437406Z","iopub.status.idle":"2025-11-27T14:43:20.437807Z","shell.execute_reply.started":"2025-11-27T14:43:20.437639Z","shell.execute_reply":"2025-11-27T14:43:20.437656Z"}},"outputs":[],"execution_count":null},{"id":"a5e4fa79","cell_type":"markdown","source":"# REINFORCE","metadata":{}},{"id":"bf4207d3","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}