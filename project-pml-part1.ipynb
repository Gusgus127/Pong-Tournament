{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Part 1: Pong Tournament\n## **DQN and REINFORCE implementation on Pong environment**","metadata":{}},{"cell_type":"code","source":"!pip install gymnasium[atari]\n!pip install gymnasium[accept-rom-license]\n!pip install ale-py\n!pip install autorom\n!AutoROM --accept-license","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:00:47.002358Z","iopub.execute_input":"2025-12-02T00:00:47.002817Z","iopub.status.idle":"2025-12-02T00:01:13.007840Z","shell.execute_reply.started":"2025-12-02T00:00:47.002793Z","shell.execute_reply":"2025-12-02T00:01:13.007072Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (0.29.0)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\nCollecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (2.4.1)\nCollecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n  Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.5.2)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[atari]) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[atari]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[atari]) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium[atari]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium[atari]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium[atari]) (2024.2.0)\nDownloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\nDownloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: shimmy, ale-py\n  Attempting uninstall: shimmy\n    Found existing installation: Shimmy 1.3.0\n    Uninstalling Shimmy-1.3.0:\n      Successfully uninstalled Shimmy-1.3.0\n  Attempting uninstall: ale-py\n    Found existing installation: ale-py 0.11.2\n    Uninstalling ale-py-0.11.2:\n      Successfully uninstalled ale-py-0.11.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.18.0 requires shimmy>=1.2.1, but you have shimmy 0.2.1 which is incompatible.\ndopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ale-py-0.8.1 shimmy-0.2.1\nRequirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.29.0)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (3.1.2)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\nCollecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.67.1)\nCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[accept-rom-license]) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium[accept-rom-license]) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2025.10.5)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium[accept-rom-license]) (2024.2.0)\nDownloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: AutoROM.accept-rom-license\n  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446709 sha256=107248ed733925bffdaeeed1b346666df7507e46568f180c1475e2e1d3e881d6\n  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\nSuccessfully built AutoROM.accept-rom-license\nInstalling collected packages: AutoROM.accept-rom-license, autorom\nSuccessfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\nRequirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.8.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ale-py) (1.26.4)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py) (6.5.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->ale-py) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ale-py) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ale-py) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ale-py) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->ale-py) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->ale-py) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->ale-py) (2024.2.0)\nRequirement already satisfied: autorom in /usr/local/lib/python3.11/dist-packages (0.4.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom) (8.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom) (4.67.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2025.10.5)\nAutoROM will download the Atari 2600 ROMs.\nThey will be installed to:\n\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n\nExisting ROMs will be overwritten.\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/adventure.bin    \nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/air_raid.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/alien.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/amidar.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/assault.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asterix.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asteroids.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis2.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/backgammon.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bank_heist.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/basic_math.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/battle_zone.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/beam_rider.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/berzerk.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/blackjack.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bowling.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/boxing.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/breakout.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/carnival.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/casino.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/centipede.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/chopper_command.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/combat.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crazy_climber.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crossbow.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/darkchambers.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/defender.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/demon_attack.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/donkey_kong.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/double_dunk.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/earthworld.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/elevator_action.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/enduro.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/entombed.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/et.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/fishing_derby.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/flag_capture.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/freeway.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frogger.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frostbite.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/galaxian.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gopher.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gravitar.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hangman.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/haunted_house.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hero.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/human_cannonball.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ice_hockey.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/jamesbond.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/journey_escape.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/joust.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kaboom.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kangaroo.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/keystone_kapers.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/king_kong.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/klax.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/koolaid.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/krull.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kung_fu_master.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/laser_gates.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/lost_luggage.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mario_bros.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/maze_craze.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/miniature_golf.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/montezuma_revenge.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mr_do.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ms_pacman.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/name_this_game.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/othello.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pacman.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/phoenix.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall2.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pong.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pooyan.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/private_eye.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/qbert.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/riverraid.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/road_runner.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/robotank.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/seaquest.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/sir_lancelot.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/skiing.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/solaris.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_invaders.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_war.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/star_gunner.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/superman.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/surround.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tennis.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tetris.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/time_pilot.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/trondead.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/turmoil.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tutankham.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/up_n_down.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/venture.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_checkers.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_chess.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_cube.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_pinball.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/warlords.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/wizard_of_wor.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/word_zapper.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/yars_revenge.bin\nInstalled /usr/local/lib/python3.11/dist-packages/AutoROM/roms/zaxxon.bin\nDone!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Import necessary libraries","metadata":{}},{"cell_type":"code","source":"import gymnasium as gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport collections\nimport random\nimport time\nimport datetime\nfrom collections import deque, namedtuple\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:01:13.009545Z","iopub.execute_input":"2025-12-02T00:01:13.009766Z","iopub.status.idle":"2025-12-02T00:01:19.888162Z","shell.execute_reply.started":"2025-12-02T00:01:13.009742Z","shell.execute_reply":"2025-12-02T00:01:19.887620Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\nprint(f\"Using Gymnasium version: {gym.__version__}\")\nprint(f\"Using PyTorch version: {torch.__version__}\")\n\n# Define the Experience tuple\nExperience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'new_state', 'done'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:01:19.889190Z","iopub.execute_input":"2025-12-02T00:01:19.889559Z","iopub.status.idle":"2025-12-02T00:01:19.979749Z","shell.execute_reply.started":"2025-12-02T00:01:19.889531Z","shell.execute_reply":"2025-12-02T00:01:19.979143Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nUsing Gymnasium version: 0.29.0\nUsing PyTorch version: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Data preprocessing (Wrappers)\nWe need to apply the same set of wrappers used during the model's training phase to ensure that the inputs to the model are consistent in shape, format, and meaning.","metadata":{}},{"cell_type":"code","source":"# Fixed Environment Wrappers for Gymnasium\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        # Check if fire action is available - handle different environments gracefully\n        action_meanings = env.unwrapped.get_action_meanings()\n        if len(action_meanings) > 1:\n            self._has_fire_action = action_meanings[1] == 'FIRE'\n        else:\n            self._has_fire_action = False\n\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        if self._has_fire_action:\n            obs, _, terminated, truncated, _ = self.env.step(1)\n            if terminated or truncated:\n                obs, info = self.env.reset(**kwargs)\n            obs, _, terminated, truncated, _ = self.env.step(2)\n            if terminated or truncated:\n                obs, info = self.env.reset(**kwargs)\n        return obs, info\n\n    def step(self, action):\n        return self.env.step(action)\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        super().__init__(env)\n        self._obs_buffer = collections.deque(maxlen=2)\n        self._skip = skip\n\n    def step(self, action):\n        total_reward = 0.0\n        terminated = False\n        truncated = False\n        info = {}\n        \n        for _ in range(self._skip):\n            obs, reward, terminated, truncated, info = self.env.step(action)\n            self._obs_buffer.append(obs)\n            total_reward += reward\n            if terminated or truncated:\n                break\n        \n        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n        return max_frame, total_reward, terminated, truncated, info\n\n    def reset(self, **kwargs):\n        self._obs_buffer.clear()\n        obs, info = self.env.reset(**kwargs)\n        self._obs_buffer.append(obs)\n        return obs, info\n\nclass ProcessFrame84(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n\n    def observation(self, obs):\n        return self._process_frame(obs)\n\n    def _process_frame(self, frame):\n        if frame.size == 210 * 160 * 3:\n            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n        elif frame.size == 250 * 160 * 3:\n            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n        else:\n            # Try to handle different frame sizes\n            img = frame.astype(np.float32)\n            if len(img.shape) == 3 and img.shape[2] == 3:\n                pass  # Already in correct format\n            else:\n                # Assume standard Atari frame shape\n                try:\n                    img = img.reshape((210, 160, 3))\n                except:\n                    # If reshaping fails, use as is and hope for the best\n                    pass\n        \n        # Convert to grayscale\n        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n        \n        # Resize\n        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n        x_t = resized_screen[18:102, :]\n        x_t = np.reshape(x_t, [84, 84, 1])\n        return x_t.astype(np.uint8)\n\nclass BufferWrapper(gym.ObservationWrapper):\n    def __init__(self, env, n_steps):\n        super().__init__(env)\n        self.n_steps = n_steps\n        old_space = env.observation_space\n        \n        # Fix: Properly handle the observation space dimensions\n        new_shape = (old_space.shape[0], old_space.shape[1], old_space.shape[2] * n_steps)\n\n        self.observation_space = gym.spaces.Box(\n            low=0.0, high=1.0,\n            shape=new_shape,\n            dtype=np.float32\n        )\n\n        self.buffer = deque(maxlen=n_steps)\n\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        for _ in range(self.n_steps):\n            self.buffer.append(obs)\n        return self._get_obs(), info\n\n    def observation(self, obs):\n        self.buffer.append(obs)\n        return self._get_obs()\n\n    def _get_obs(self):\n        # Stack along channel axis (axis=2 for HWC format)\n        return np.concatenate(list(self.buffer), axis=2) \n\nclass ImageToPyTorch(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        old_shape = self.observation_space.shape\n        # Fix: Change from HWC to CHW format\n        self.observation_space = gym.spaces.Box(\n            low=0.0, high=1.0, shape=(old_shape[2], old_shape[0], old_shape[1]), dtype=np.float32\n        )\n\n    def observation(self, obs):\n        # Fix: Properly transpose from HWC to CHW\n        return np.transpose(obs, (2, 0, 1))\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def observation(self, obs):\n        return np.array(obs, dtype=np.float32) / 255.0\n\n\n# Create environment function\ndef make_env(env_name):\n    env = gym.make(env_name, render_mode='rgb_array')\n    env = MaxAndSkipEnv(env)\n    env = FireResetEnv(env)\n    env = ProcessFrame84(env)\n    env = ImageToPyTorch(env)\n    env = BufferWrapper(env, 4)\n    env = ScaledFloatFrame(env)\n    return env","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:01:19.981570Z","iopub.execute_input":"2025-12-02T00:01:19.982161Z","iopub.status.idle":"2025-12-02T00:01:20.042420Z","shell.execute_reply.started":"2025-12-02T00:01:19.982130Z","shell.execute_reply":"2025-12-02T00:01:20.041657Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Test the environment wrappers","metadata":{}},{"cell_type":"code","source":"ENV_NAME = \"PongNoFrameskip-v4\"\ntest_env = make_env(ENV_NAME)\n\n# Print environment info\nprint(\"*** Test Environment ***\")\nprint(f\"Observation space shape: {test_env.observation_space.shape}\")\nprint(f\"Observation space type: {test_env.observation_space.dtype}\")\nprint(f\"Action space: {test_env.action_space}\")\nprint(f\"Number of actions: {test_env.action_space.n}\")\n\n# Test reset\nobs, info = test_env.reset()\nprint(f\"\\nInitial observation shape: {obs.shape}\")\nprint(f\"Initial observation range: [{obs.min()}, {obs.max()}]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:01:20.043055Z","iopub.execute_input":"2025-12-02T00:01:20.043296Z","iopub.status.idle":"2025-12-02T00:01:20.382635Z","shell.execute_reply.started":"2025-12-02T00:01:20.043280Z","shell.execute_reply":"2025-12-02T00:01:20.382058Z"}},"outputs":[{"name":"stderr","text":"A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n[Powered by Stella]\n","output_type":"stream"},{"name":"stdout","text":"*** Test Environment ***\nObservation space shape: (1, 84, 336)\nObservation space type: float32\nAction space: Discrete(6)\nNumber of actions: 6\n\nInitial observation shape: (1, 84, 336)\nInitial observation range: [0.34117648005485535, 0.6352941393852234]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **DQN**","metadata":{}},{"cell_type":"code","source":"# DQN Network\nclass DQN(nn.Module):\n    def __init__(self, input_shape, n_actions):\n        super(DQN, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU(),\n        )\n\n        # Calculate the size of the flattened features\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, *input_shape)\n            conv_out = self.conv(dummy_input)\n            n_flatten = conv_out.view(1, -1).size(1)\n\n        self.fc = nn.Sequential(\n            nn.Linear(n_flatten, 512),\n            nn.ReLU(),\n            nn.Linear(512, n_actions)\n        )\n\n    def forward(self, x):\n        # x shape: [batch_size, channels, height, width]\n        conv_out = self.conv(x)\n        flattened = conv_out.view(conv_out.size(0), -1)\n        return self.fc(flattened)\n\n# Test the network\ntest_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\nprint(f\"Network architecture: {test_net}\")\nprint(f\"Number of parameters: {sum(p.numel() for p in test_net.parameters())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:01:20.383331Z","iopub.execute_input":"2025-12-02T00:01:20.383573Z","iopub.status.idle":"2025-12-02T00:01:20.907025Z","shell.execute_reply.started":"2025-12-02T00:01:20.383553Z","shell.execute_reply":"2025-12-02T00:01:20.906389Z"}},"outputs":[{"name":"stdout","text":"Network architecture: DQN(\n  (conv): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU()\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=17024, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=6, bias=True)\n  )\n)\nNumber of parameters: 8791718\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Experience Buffer\nclass ExperienceBuffer:\n    def __init__(self, capacity):\n        self.buffer = collections.deque(maxlen=capacity)\n\n    def __len__(self):\n        return len(self.buffer)\n\n    def append(self, experience):\n        self.buffer.append(experience)\n\n    def sample(self, batch_size):\n        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n\n        return (\n            np.array(states, dtype=np.float32),\n            np.array(actions, dtype=np.int64),\n            np.array(rewards, dtype=np.float32),\n            np.array(next_states, dtype=np.float32),\n            np.array(dones, dtype=np.bool_)\n        )\n\n# Agent\nclass Agent:\n    def __init__(self, env, exp_buffer):\n        self.env = env\n        self.exp_buffer = exp_buffer\n        self.reset()\n\n    def reset(self):\n        self.state, _ = self.env.reset()\n        self.total_reward = 0.0\n\n    @torch.no_grad()\n    def play_step(self, net, epsilon, device=\"cpu\"):\n        done_reward = None\n    \n        if np.random.random() < epsilon:\n            action = self.env.action_space.sample()\n        else:\n            state_v = torch.tensor(np.array([self.state], copy=False), dtype=torch.float32).to(device)\n            q_vals = net(state_v)\n            _, act_v = torch.max(q_vals, dim=1)\n            action = int(act_v.item())\n    \n        new_state, reward, terminated, truncated, _ = self.env.step(action)\n        done = terminated or truncated\n    \n        exp = Experience(self.state, action, reward, new_state, done)\n        self.exp_buffer.append(exp)\n    \n        self.state = new_state\n        self.total_reward += reward\n    \n        if done:\n            done_reward = self.total_reward\n            self.reset()\n    \n        return done_reward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:01:20.907733Z","iopub.execute_input":"2025-12-02T00:01:20.907993Z","iopub.status.idle":"2025-12-02T00:01:20.916584Z","shell.execute_reply.started":"2025-12-02T00:01:20.907975Z","shell.execute_reply":"2025-12-02T00:01:20.915875Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Loss calculation functions\ndef calc_loss(batch, net, tgt_net, device, gamma):\n    states, actions, rewards, next_states, dones = batch\n\n    states_v = torch.tensor(states, dtype=torch.float32).to(device)\n    next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)\n    actions_v = torch.tensor(actions, dtype=torch.int64).to(device)\n    rewards_v = torch.tensor(rewards, dtype=torch.float32).to(device)\n    done_mask = torch.tensor(dones, dtype=torch.bool).to(device)\n\n    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n\n    with torch.no_grad():\n        next_state_values = tgt_net(next_states_v).max(1)[0]\n        next_state_values[done_mask] = 0.0\n        expected_state_action_values = rewards_v + gamma * next_state_values\n\n    return nn.MSELoss()(state_action_values, expected_state_action_values)\n\n# Plotting function\ndef plot_rewards(rewards, ma_window=100):\n    plt.figure(figsize=(12, 8))\n    plt.title(\"Training Rewards\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Reward\")\n    \n    if len(rewards) > 0:\n        plt.plot(rewards, alpha=0.3, label='Raw rewards', color='blue')\n        \n        if len(rewards) >= ma_window:\n            ma_rewards = []\n            for i in range(len(rewards) - ma_window + 1):\n                ma_rewards.append(np.mean(rewards[i:i+ma_window]))\n            plt.plot(range(ma_window-1, len(rewards)), ma_rewards, \n                    label=f'Moving Average ({ma_window} episodes)', color='red', linewidth=2)\n        \n        plt.legend()\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:01:20.917347Z","iopub.execute_input":"2025-12-02T00:01:20.917590Z","iopub.status.idle":"2025-12-02T00:01:20.944939Z","shell.execute_reply.started":"2025-12-02T00:01:20.917563Z","shell.execute_reply":"2025-12-02T00:01:20.944472Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Training function\ndef train_dqn():\n    # Create models directory\n    os.makedirs(\"models\", exist_ok=True)\n    \n    # Parameters\n    ENV_NAME = \"PongNoFrameskip-v4\"\n    MEAN_REWARD_BOUND = 18.0  # Pong is solved when average reward > 18\n    \n    GAMMA = 0.99\n    BATCH_SIZE = 32\n    REPLAY_SIZE = 10000\n    LEARNING_RATE = 1e-4\n    SYNC_TARGET_FRAMES = 1000  # Reduced for faster updates\n    REPLAY_START_SIZE = 10000  # Increased for more initial experiences\n    \n    EPSILON_DECAY_LAST_FRAME = 100000\n    EPSILON_START = 1.0\n    EPSILON_FINAL = 0.02\n    \n    print(f\"Training DQN on {ENV_NAME}\")\n    print(f\"Device: {device}\")\n    \n    # Create environment\n    env = make_env(ENV_NAME)\n\n    # Initialize networks\n    net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n    tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n    tgt_net.load_state_dict(net.state_dict())\n    \n    print(f\"Network: {net}\")\n    print(f\"Input shape: {env.observation_space.shape}\")\n    print(f\"Number of actions: {env.action_space.n}\")\n    \n    # Initialize buffer and agent\n    buffer = ExperienceBuffer(REPLAY_SIZE)\n    agent = Agent(env, buffer)\n    \n    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n    total_rewards = []\n    frame_idx = 0\n    ts_frame = 0\n    ts = time.time()\n    best_mean_reward = None\n    \n    print(\"Starting training at:\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n    print(\"Filling replay buffer...\")\n    \n    # Initial filling of replay buffer\n    while len(buffer) < REPLAY_START_SIZE:\n        frame_idx += 1\n        epsilon = 1.0  # Always explore during initial filling\n        reward = agent.play_step(net, epsilon, device)\n        if reward is not None:\n            total_rewards.append(reward)\n            \n        if frame_idx % 1000 == 0:\n            print(f\"Filled {len(buffer)}/{REPLAY_START_SIZE} experiences in replay buffer\")\n    \n    print(\"Replay buffer filled. Starting training...\")\n    \n    episode_count = 0\n    \n    max_frames = 500000\n    \n    while frame_idx < max_frames:\n        frame_idx += 1\n        \n        # Calculate epsilon\n        epsilon = max(EPSILON_FINAL, \n                     EPSILON_START - (EPSILON_START - EPSILON_FINAL) * frame_idx / EPSILON_DECAY_LAST_FRAME)\n        \n        reward = agent.play_step(net, epsilon, device)\n        if reward is not None:\n            total_rewards.append(reward)\n            episode_count += 1\n            \n            # Calculate mean reward over last 100 episodes\n            mean_reward = np.mean(total_rewards[-100:]) if len(total_rewards) >= 100 else np.mean(total_rewards)\n            \n            # Calculate speed\n            speed = (frame_idx - ts_frame) / (time.time() - ts)\n            ts_frame = frame_idx\n            ts = time.time()\n            \n            # Print progress every 10 episodes\n            if episode_count % 10 == 0:\n                print(f\"Episode {episode_count}, Frame {frame_idx}, Reward: {reward:.1f}, \"\n                      f\"Mean Reward (last 100): {mean_reward:.3f}, Epsilon: {epsilon:.3f}, Speed: {speed:.2f} f/s\")\n            \n            # Save best model\n            if best_mean_reward is None or mean_reward > best_mean_reward:\n                if best_mean_reward is not None:\n                    print(f\"New best mean reward: {best_mean_reward:.3f} -> {mean_reward:.3f}\")\n                best_mean_reward = mean_reward\n                model_path = f\"models/{ENV_NAME}_best_{mean_reward:.2f}.pth\"\n                torch.save(net.state_dict(), model_path)\n                print(f\"Model saved to {model_path}\")\n            \n            # Check if solved\n            if mean_reward >= MEAN_REWARD_BOUND:\n                print(f\"Solved at frame {frame_idx} with mean reward {mean_reward:.3f}!\")\n                break\n        \n        # Sync target network\n        if frame_idx % SYNC_TARGET_FRAMES == 0:\n            tgt_net.load_state_dict(net.state_dict())\n            print(f\"Target network updated at frame {frame_idx}\")\n        \n        # Training step - only train if we have enough samples\n        if len(buffer) >= BATCH_SIZE:\n            optimizer.zero_grad()\n            batch = buffer.sample(BATCH_SIZE)\n            loss_t = calc_loss(batch, net, tgt_net, device, GAMMA)\n            loss_t.backward()\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n            optimizer.step()\n            \n    print(\"Training completed at:\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n    print(f\"Trained for {frame_idx} frames, {episode_count} episodes\")\n    \n    # Save final model\n    final_model_path = f\"models/{ENV_NAME}_final.pth\"\n    torch.save(net.state_dict(), final_model_path)\n    print(f\"Final model saved to {final_model_path}\")\n    \n    return total_rewards, net","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:01:20.945601Z","iopub.execute_input":"2025-12-02T00:01:20.946426Z","iopub.status.idle":"2025-12-02T00:01:20.971667Z","shell.execute_reply.started":"2025-12-02T00:01:20.946398Z","shell.execute_reply":"2025-12-02T00:01:20.971134Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Start the training\nprint(\"=\" * 60)\nprint(\"STARTING DQN TRAINING ON PONG\")\nprint(\"=\" * 60)\n\nrewards, trained_net = train_dqn()\n\nprint(\"=\" * 60)\nprint(\"TRAINING COMPLETED\")\nprint(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T00:01:20.973205Z","iopub.execute_input":"2025-12-02T00:01:20.973402Z","iopub.status.idle":"2025-12-02T02:05:14.932378Z","shell.execute_reply.started":"2025-12-02T00:01:20.973388Z","shell.execute_reply":"2025-12-02T02:05:14.931359Z"}},"outputs":[{"name":"stdout","text":"============================================================\nSTARTING DQN TRAINING ON PONG\n============================================================\nTraining DQN on PongNoFrameskip-v4\nDevice: cuda\nNetwork: DQN(\n  (conv): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU()\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=17024, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=6, bias=True)\n  )\n)\nInput shape: (1, 84, 336)\nNumber of actions: 6\nStarting training at: 2025-12-02 00:01:24\nFilling replay buffer...\nFilled 1000/10000 experiences in replay buffer\nFilled 2000/10000 experiences in replay buffer\nFilled 3000/10000 experiences in replay buffer\nFilled 4000/10000 experiences in replay buffer\nFilled 5000/10000 experiences in replay buffer\nFilled 6000/10000 experiences in replay buffer\nFilled 7000/10000 experiences in replay buffer\nFilled 8000/10000 experiences in replay buffer\nFilled 9000/10000 experiences in replay buffer\nFilled 10000/10000 experiences in replay buffer\nReplay buffer filled. Starting training...\nModel saved to models/PongNoFrameskip-v4_best_-20.00.pth\nTarget network updated at frame 11000\nTarget network updated at frame 12000\nTarget network updated at frame 13000\nTarget network updated at frame 14000\nTarget network updated at frame 15000\nTarget network updated at frame 16000\nTarget network updated at frame 17000\nTarget network updated at frame 18000\nEpisode 10, Frame 18152, Reward: -21.0, Mean Reward (last 100): -20.300, Epsilon: 0.822, Speed: 95.08 f/s\nTarget network updated at frame 19000\nTarget network updated at frame 20000\nTarget network updated at frame 21000\nTarget network updated at frame 22000\nTarget network updated at frame 23000\nTarget network updated at frame 24000\nTarget network updated at frame 25000\nTarget network updated at frame 26000\nTarget network updated at frame 27000\nTarget network updated at frame 28000\nEpisode 20, Frame 28110, Reward: -19.0, Mean Reward (last 100): -20.200, Epsilon: 0.725, Speed: 91.42 f/s\nTarget network updated at frame 29000\nTarget network updated at frame 30000\nTarget network updated at frame 31000\nTarget network updated at frame 32000\nTarget network updated at frame 33000\nTarget network updated at frame 34000\nTarget network updated at frame 35000\nTarget network updated at frame 36000\nTarget network updated at frame 37000\nTarget network updated at frame 38000\nEpisode 30, Frame 38973, Reward: -18.0, Mean Reward (last 100): -20.200, Epsilon: 0.618, Speed: 88.41 f/s\nTarget network updated at frame 39000\nTarget network updated at frame 40000\nTarget network updated at frame 41000\nTarget network updated at frame 42000\nTarget network updated at frame 43000\nTarget network updated at frame 44000\nTarget network updated at frame 45000\nTarget network updated at frame 46000\nTarget network updated at frame 47000\nTarget network updated at frame 48000\nTarget network updated at frame 49000\nTarget network updated at frame 50000\nEpisode 40, Frame 50331, Reward: -19.0, Mean Reward (last 100): -20.100, Epsilon: 0.507, Speed: 82.23 f/s\nTarget network updated at frame 51000\nTarget network updated at frame 52000\nTarget network updated at frame 53000\nTarget network updated at frame 54000\nTarget network updated at frame 55000\nTarget network updated at frame 56000\nTarget network updated at frame 57000\nTarget network updated at frame 58000\nTarget network updated at frame 59000\nTarget network updated at frame 60000\nTarget network updated at frame 61000\nNew best mean reward: -20.000 -> -19.966\nModel saved to models/PongNoFrameskip-v4_best_-19.97.pth\nTarget network updated at frame 62000\nTarget network updated at frame 63000\nNew best mean reward: -19.966 -> -19.915\nModel saved to models/PongNoFrameskip-v4_best_-19.92.pth\nTarget network updated at frame 64000\nEpisode 50, Frame 64971, Reward: -18.0, Mean Reward (last 100): -19.883, Epsilon: 0.363, Speed: 81.24 f/s\nNew best mean reward: -19.915 -> -19.883\nModel saved to models/PongNoFrameskip-v4_best_-19.88.pth\nTarget network updated at frame 65000\nTarget network updated at frame 66000\nTarget network updated at frame 67000\nTarget network updated at frame 68000\nNew best mean reward: -19.883 -> -19.839\nModel saved to models/PongNoFrameskip-v4_best_-19.84.pth\nTarget network updated at frame 69000\nTarget network updated at frame 70000\nNew best mean reward: -19.839 -> -19.794\nModel saved to models/PongNoFrameskip-v4_best_-19.79.pth\nTarget network updated at frame 71000\nTarget network updated at frame 72000\nTarget network updated at frame 73000\nTarget network updated at frame 74000\nTarget network updated at frame 75000\nNew best mean reward: -19.794 -> -19.742\nModel saved to models/PongNoFrameskip-v4_best_-19.74.pth\nTarget network updated at frame 76000\nTarget network updated at frame 77000\nTarget network updated at frame 78000\nNew best mean reward: -19.742 -> -19.676\nModel saved to models/PongNoFrameskip-v4_best_-19.68.pth\nTarget network updated at frame 79000\nTarget network updated at frame 80000\nNew best mean reward: -19.676 -> -19.609\nModel saved to models/PongNoFrameskip-v4_best_-19.61.pth\nTarget network updated at frame 81000\nTarget network updated at frame 82000\nEpisode 60, Frame 82162, Reward: -15.0, Mean Reward (last 100): -19.543, Epsilon: 0.195, Speed: 77.94 f/s\nNew best mean reward: -19.609 -> -19.543\nModel saved to models/PongNoFrameskip-v4_best_-19.54.pth\nTarget network updated at frame 83000\nNew best mean reward: -19.543 -> -19.493\nModel saved to models/PongNoFrameskip-v4_best_-19.49.pth\nTarget network updated at frame 84000\nTarget network updated at frame 85000\nNew best mean reward: -19.493 -> -19.444\nModel saved to models/PongNoFrameskip-v4_best_-19.44.pth\nTarget network updated at frame 86000\nTarget network updated at frame 87000\nTarget network updated at frame 88000\nTarget network updated at frame 89000\nNew best mean reward: -19.444 -> -19.392\nModel saved to models/PongNoFrameskip-v4_best_-19.39.pth\nTarget network updated at frame 90000\nTarget network updated at frame 91000\nTarget network updated at frame 92000\nTarget network updated at frame 93000\nTarget network updated at frame 94000\nNew best mean reward: -19.392 -> -19.338\nModel saved to models/PongNoFrameskip-v4_best_-19.34.pth\nTarget network updated at frame 95000\nTarget network updated at frame 96000\nNew best mean reward: -19.338 -> -19.295\nModel saved to models/PongNoFrameskip-v4_best_-19.29.pth\nTarget network updated at frame 97000\nTarget network updated at frame 98000\nNew best mean reward: -19.295 -> -19.278\nModel saved to models/PongNoFrameskip-v4_best_-19.28.pth\nTarget network updated at frame 99000\nTarget network updated at frame 100000\nEpisode 70, Frame 100337, Reward: -13.0, Mean Reward (last 100): -19.200, Epsilon: 0.020, Speed: 76.19 f/s\nNew best mean reward: -19.278 -> -19.200\nModel saved to models/PongNoFrameskip-v4_best_-19.20.pth\nTarget network updated at frame 101000\nTarget network updated at frame 102000\nNew best mean reward: -19.200 -> -19.173\nModel saved to models/PongNoFrameskip-v4_best_-19.17.pth\nTarget network updated at frame 103000\nTarget network updated at frame 104000\nNew best mean reward: -19.173 -> -19.134\nModel saved to models/PongNoFrameskip-v4_best_-19.13.pth\nTarget network updated at frame 105000\nTarget network updated at frame 106000\nNew best mean reward: -19.134 -> -19.084\nModel saved to models/PongNoFrameskip-v4_best_-19.08.pth\nTarget network updated at frame 107000\nTarget network updated at frame 108000\nNew best mean reward: -19.084 -> -19.060\nModel saved to models/PongNoFrameskip-v4_best_-19.06.pth\nTarget network updated at frame 109000\nNew best mean reward: -19.060 -> -19.059\nModel saved to models/PongNoFrameskip-v4_best_-19.06.pth\nTarget network updated at frame 110000\nTarget network updated at frame 111000\nNew best mean reward: -19.059 -> -18.988\nModel saved to models/PongNoFrameskip-v4_best_-18.99.pth\nTarget network updated at frame 112000\nTarget network updated at frame 113000\nNew best mean reward: -18.988 -> -18.966\nModel saved to models/PongNoFrameskip-v4_best_-18.97.pth\nTarget network updated at frame 114000\nTarget network updated at frame 115000\nNew best mean reward: -18.966 -> -18.841\nModel saved to models/PongNoFrameskip-v4_best_-18.84.pth\nTarget network updated at frame 116000\nTarget network updated at frame 117000\nNew best mean reward: -18.841 -> -18.798\nModel saved to models/PongNoFrameskip-v4_best_-18.80.pth\nTarget network updated at frame 118000\nEpisode 80, Frame 118764, Reward: -19.0, Mean Reward (last 100): -18.800, Epsilon: 0.020, Speed: 71.41 f/s\nTarget network updated at frame 119000\nTarget network updated at frame 120000\nNew best mean reward: -18.798 -> -18.791\nModel saved to models/PongNoFrameskip-v4_best_-18.79.pth\nTarget network updated at frame 121000\nNew best mean reward: -18.791 -> -18.772\nModel saved to models/PongNoFrameskip-v4_best_-18.77.pth\nTarget network updated at frame 122000\nTarget network updated at frame 123000\nTarget network updated at frame 124000\nNew best mean reward: -18.772 -> -18.710\nModel saved to models/PongNoFrameskip-v4_best_-18.71.pth\nTarget network updated at frame 125000\nTarget network updated at frame 126000\nNew best mean reward: -18.710 -> -18.702\nModel saved to models/PongNoFrameskip-v4_best_-18.70.pth\nTarget network updated at frame 127000\nTarget network updated at frame 128000\nNew best mean reward: -18.702 -> -18.674\nModel saved to models/PongNoFrameskip-v4_best_-18.67.pth\nTarget network updated at frame 129000\nTarget network updated at frame 130000\nTarget network updated at frame 131000\nTarget network updated at frame 132000\nNew best mean reward: -18.674 -> -18.639\nModel saved to models/PongNoFrameskip-v4_best_-18.64.pth\nTarget network updated at frame 133000\nTarget network updated at frame 134000\nTarget network updated at frame 135000\nTarget network updated at frame 136000\nNew best mean reward: -18.639 -> -18.576\nModel saved to models/PongNoFrameskip-v4_best_-18.58.pth\nTarget network updated at frame 137000\nTarget network updated at frame 138000\nEpisode 90, Frame 138506, Reward: -13.0, Mean Reward (last 100): -18.520, Epsilon: 0.020, Speed: 71.63 f/s\nNew best mean reward: -18.576 -> -18.520\nModel saved to models/PongNoFrameskip-v4_best_-18.52.pth\nTarget network updated at frame 139000\nNew best mean reward: -18.520 -> -18.500\nModel saved to models/PongNoFrameskip-v4_best_-18.50.pth\nTarget network updated at frame 140000\nTarget network updated at frame 141000\nTarget network updated at frame 142000\nNew best mean reward: -18.500 -> -18.430\nModel saved to models/PongNoFrameskip-v4_best_-18.43.pth\nTarget network updated at frame 143000\nTarget network updated at frame 144000\nNew best mean reward: -18.430 -> -18.350\nModel saved to models/PongNoFrameskip-v4_best_-18.35.pth\nTarget network updated at frame 145000\nTarget network updated at frame 146000\nTarget network updated at frame 147000\nNew best mean reward: -18.350 -> -18.230\nModel saved to models/PongNoFrameskip-v4_best_-18.23.pth\nTarget network updated at frame 148000\nTarget network updated at frame 149000\nTarget network updated at frame 150000\nNew best mean reward: -18.230 -> -18.130\nModel saved to models/PongNoFrameskip-v4_best_-18.13.pth\nTarget network updated at frame 151000\nTarget network updated at frame 152000\nNew best mean reward: -18.130 -> -18.040\nModel saved to models/PongNoFrameskip-v4_best_-18.04.pth\nTarget network updated at frame 153000\nTarget network updated at frame 154000\nNew best mean reward: -18.040 -> -17.990\nModel saved to models/PongNoFrameskip-v4_best_-17.99.pth\nTarget network updated at frame 155000\nTarget network updated at frame 156000\nNew best mean reward: -17.990 -> -17.880\nModel saved to models/PongNoFrameskip-v4_best_-17.88.pth\nTarget network updated at frame 157000\nTarget network updated at frame 158000\nNew best mean reward: -17.880 -> -17.830\nModel saved to models/PongNoFrameskip-v4_best_-17.83.pth\nTarget network updated at frame 159000\nTarget network updated at frame 160000\nTarget network updated at frame 161000\nEpisode 100, Frame 161502, Reward: -8.0, Mean Reward (last 100): -17.700, Epsilon: 0.020, Speed: 71.15 f/s\nNew best mean reward: -17.830 -> -17.700\nModel saved to models/PongNoFrameskip-v4_best_-17.70.pth\nTarget network updated at frame 162000\nTarget network updated at frame 163000\nTarget network updated at frame 164000\nNew best mean reward: -17.700 -> -17.560\nModel saved to models/PongNoFrameskip-v4_best_-17.56.pth\nTarget network updated at frame 165000\nTarget network updated at frame 166000\nTarget network updated at frame 167000\nNew best mean reward: -17.560 -> -17.360\nModel saved to models/PongNoFrameskip-v4_best_-17.36.pth\nTarget network updated at frame 168000\nTarget network updated at frame 169000\nTarget network updated at frame 170000\nTarget network updated at frame 171000\nNew best mean reward: -17.360 -> -17.120\nModel saved to models/PongNoFrameskip-v4_best_-17.12.pth\nTarget network updated at frame 172000\nTarget network updated at frame 173000\nTarget network updated at frame 174000\nNew best mean reward: -17.120 -> -16.880\nModel saved to models/PongNoFrameskip-v4_best_-16.88.pth\nTarget network updated at frame 175000\nTarget network updated at frame 176000\nTarget network updated at frame 177000\nTarget network updated at frame 178000\nNew best mean reward: -16.880 -> -16.700\nModel saved to models/PongNoFrameskip-v4_best_-16.70.pth\nTarget network updated at frame 179000\nTarget network updated at frame 180000\nTarget network updated at frame 181000\nNew best mean reward: -16.700 -> -16.410\nModel saved to models/PongNoFrameskip-v4_best_-16.41.pth\nTarget network updated at frame 182000\nTarget network updated at frame 183000\nTarget network updated at frame 184000\nNew best mean reward: -16.410 -> -16.170\nModel saved to models/PongNoFrameskip-v4_best_-16.17.pth\nTarget network updated at frame 185000\nTarget network updated at frame 186000\nTarget network updated at frame 187000\nNew best mean reward: -16.170 -> -15.910\nModel saved to models/PongNoFrameskip-v4_best_-15.91.pth\nTarget network updated at frame 188000\nTarget network updated at frame 189000\nTarget network updated at frame 190000\nNew best mean reward: -15.910 -> -15.640\nModel saved to models/PongNoFrameskip-v4_best_-15.64.pth\nTarget network updated at frame 191000\nTarget network updated at frame 192000\nTarget network updated at frame 193000\nEpisode 110, Frame 193341, Reward: 9.0, Mean Reward (last 100): -15.340, Epsilon: 0.020, Speed: 71.45 f/s\nNew best mean reward: -15.640 -> -15.340\nModel saved to models/PongNoFrameskip-v4_best_-15.34.pth\nTarget network updated at frame 194000\nTarget network updated at frame 195000\nTarget network updated at frame 196000\nNew best mean reward: -15.340 -> -15.070\nModel saved to models/PongNoFrameskip-v4_best_-15.07.pth\nTarget network updated at frame 197000\nTarget network updated at frame 198000\nTarget network updated at frame 199000\nNew best mean reward: -15.070 -> -14.870\nModel saved to models/PongNoFrameskip-v4_best_-14.87.pth\nTarget network updated at frame 200000\nTarget network updated at frame 201000\nTarget network updated at frame 202000\nNew best mean reward: -14.870 -> -14.620\nModel saved to models/PongNoFrameskip-v4_best_-14.62.pth\nTarget network updated at frame 203000\nTarget network updated at frame 204000\nTarget network updated at frame 205000\nNew best mean reward: -14.620 -> -14.300\nModel saved to models/PongNoFrameskip-v4_best_-14.30.pth\nTarget network updated at frame 206000\nTarget network updated at frame 207000\nNew best mean reward: -14.300 -> -14.000\nModel saved to models/PongNoFrameskip-v4_best_-14.00.pth\nTarget network updated at frame 208000\nTarget network updated at frame 209000\nNew best mean reward: -14.000 -> -13.650\nModel saved to models/PongNoFrameskip-v4_best_-13.65.pth\nTarget network updated at frame 210000\nTarget network updated at frame 211000\nTarget network updated at frame 212000\nNew best mean reward: -13.650 -> -13.370\nModel saved to models/PongNoFrameskip-v4_best_-13.37.pth\nTarget network updated at frame 213000\nTarget network updated at frame 214000\nNew best mean reward: -13.370 -> -13.060\nModel saved to models/PongNoFrameskip-v4_best_-13.06.pth\nTarget network updated at frame 215000\nTarget network updated at frame 216000\nNew best mean reward: -13.060 -> -12.710\nModel saved to models/PongNoFrameskip-v4_best_-12.71.pth\nTarget network updated at frame 217000\nTarget network updated at frame 218000\nTarget network updated at frame 219000\nEpisode 120, Frame 219074, Reward: 12.0, Mean Reward (last 100): -12.400, Epsilon: 0.020, Speed: 79.86 f/s\nNew best mean reward: -12.710 -> -12.400\nModel saved to models/PongNoFrameskip-v4_best_-12.40.pth\nTarget network updated at frame 220000\nTarget network updated at frame 221000\nNew best mean reward: -12.400 -> -12.050\nModel saved to models/PongNoFrameskip-v4_best_-12.05.pth\nTarget network updated at frame 222000\nTarget network updated at frame 223000\nNew best mean reward: -12.050 -> -11.790\nModel saved to models/PongNoFrameskip-v4_best_-11.79.pth\nTarget network updated at frame 224000\nTarget network updated at frame 225000\nNew best mean reward: -11.790 -> -11.470\nModel saved to models/PongNoFrameskip-v4_best_-11.47.pth\nTarget network updated at frame 226000\nTarget network updated at frame 227000\nTarget network updated at frame 228000\nNew best mean reward: -11.470 -> -11.160\nModel saved to models/PongNoFrameskip-v4_best_-11.16.pth\nTarget network updated at frame 229000\nTarget network updated at frame 230000\nNew best mean reward: -11.160 -> -10.790\nModel saved to models/PongNoFrameskip-v4_best_-10.79.pth\nTarget network updated at frame 231000\nTarget network updated at frame 232000\nNew best mean reward: -10.790 -> -10.440\nModel saved to models/PongNoFrameskip-v4_best_-10.44.pth\nTarget network updated at frame 233000\nTarget network updated at frame 234000\nNew best mean reward: -10.440 -> -10.060\nModel saved to models/PongNoFrameskip-v4_best_-10.06.pth\nTarget network updated at frame 235000\nTarget network updated at frame 236000\nNew best mean reward: -10.060 -> -9.670\nModel saved to models/PongNoFrameskip-v4_best_-9.67.pth\nTarget network updated at frame 237000\nNew best mean reward: -9.670 -> -9.300\nModel saved to models/PongNoFrameskip-v4_best_-9.30.pth\nTarget network updated at frame 238000\nTarget network updated at frame 239000\nEpisode 130, Frame 239559, Reward: 21.0, Mean Reward (last 100): -8.910, Epsilon: 0.020, Speed: 71.97 f/s\nNew best mean reward: -9.300 -> -8.910\nModel saved to models/PongNoFrameskip-v4_best_-8.91.pth\nTarget network updated at frame 240000\nTarget network updated at frame 241000\nNew best mean reward: -8.910 -> -8.560\nModel saved to models/PongNoFrameskip-v4_best_-8.56.pth\nTarget network updated at frame 242000\nTarget network updated at frame 243000\nNew best mean reward: -8.560 -> -8.160\nModel saved to models/PongNoFrameskip-v4_best_-8.16.pth\nTarget network updated at frame 244000\nTarget network updated at frame 245000\nNew best mean reward: -8.160 -> -7.770\nModel saved to models/PongNoFrameskip-v4_best_-7.77.pth\nTarget network updated at frame 246000\nTarget network updated at frame 247000\nNew best mean reward: -7.770 -> -7.480\nModel saved to models/PongNoFrameskip-v4_best_-7.48.pth\nTarget network updated at frame 248000\nNew best mean reward: -7.480 -> -7.110\nModel saved to models/PongNoFrameskip-v4_best_-7.11.pth\nTarget network updated at frame 249000\nTarget network updated at frame 250000\nTarget network updated at frame 251000\nNew best mean reward: -7.110 -> -6.770\nModel saved to models/PongNoFrameskip-v4_best_-6.77.pth\nTarget network updated at frame 252000\nTarget network updated at frame 253000\nNew best mean reward: -6.770 -> -6.400\nModel saved to models/PongNoFrameskip-v4_best_-6.40.pth\nTarget network updated at frame 254000\nTarget network updated at frame 255000\nNew best mean reward: -6.400 -> -6.060\nModel saved to models/PongNoFrameskip-v4_best_-6.06.pth\nTarget network updated at frame 256000\nTarget network updated at frame 257000\nNew best mean reward: -6.060 -> -5.720\nModel saved to models/PongNoFrameskip-v4_best_-5.72.pth\nTarget network updated at frame 258000\nTarget network updated at frame 259000\nEpisode 140, Frame 259728, Reward: 17.0, Mean Reward (last 100): -5.360, Epsilon: 0.020, Speed: 72.48 f/s\nNew best mean reward: -5.720 -> -5.360\nModel saved to models/PongNoFrameskip-v4_best_-5.36.pth\nTarget network updated at frame 260000\nTarget network updated at frame 261000\nNew best mean reward: -5.360 -> -4.970\nModel saved to models/PongNoFrameskip-v4_best_-4.97.pth\nTarget network updated at frame 262000\nTarget network updated at frame 263000\nNew best mean reward: -4.970 -> -4.610\nModel saved to models/PongNoFrameskip-v4_best_-4.61.pth\nTarget network updated at frame 264000\nTarget network updated at frame 265000\nNew best mean reward: -4.610 -> -4.200\nModel saved to models/PongNoFrameskip-v4_best_-4.20.pth\nTarget network updated at frame 266000\nTarget network updated at frame 267000\nNew best mean reward: -4.200 -> -3.820\nModel saved to models/PongNoFrameskip-v4_best_-3.82.pth\nTarget network updated at frame 268000\nNew best mean reward: -3.820 -> -3.450\nModel saved to models/PongNoFrameskip-v4_best_-3.45.pth\nTarget network updated at frame 269000\nTarget network updated at frame 270000\nNew best mean reward: -3.450 -> -3.090\nModel saved to models/PongNoFrameskip-v4_best_-3.09.pth\nTarget network updated at frame 271000\nTarget network updated at frame 272000\nNew best mean reward: -3.090 -> -2.700\nModel saved to models/PongNoFrameskip-v4_best_-2.70.pth\nTarget network updated at frame 273000\nTarget network updated at frame 274000\nNew best mean reward: -2.700 -> -2.410\nModel saved to models/PongNoFrameskip-v4_best_-2.41.pth\nTarget network updated at frame 275000\nTarget network updated at frame 276000\nNew best mean reward: -2.410 -> -2.060\nModel saved to models/PongNoFrameskip-v4_best_-2.06.pth\nTarget network updated at frame 277000\nTarget network updated at frame 278000\nEpisode 150, Frame 278543, Reward: 16.0, Mean Reward (last 100): -1.720, Epsilon: 0.020, Speed: 73.09 f/s\nNew best mean reward: -2.060 -> -1.720\nModel saved to models/PongNoFrameskip-v4_best_-1.72.pth\nTarget network updated at frame 279000\nTarget network updated at frame 280000\nNew best mean reward: -1.720 -> -1.330\nModel saved to models/PongNoFrameskip-v4_best_-1.33.pth\nTarget network updated at frame 281000\nTarget network updated at frame 282000\nNew best mean reward: -1.330 -> -0.990\nModel saved to models/PongNoFrameskip-v4_best_-0.99.pth\nTarget network updated at frame 283000\nTarget network updated at frame 284000\nNew best mean reward: -0.990 -> -0.640\nModel saved to models/PongNoFrameskip-v4_best_-0.64.pth\nTarget network updated at frame 285000\nTarget network updated at frame 286000\nNew best mean reward: -0.640 -> -0.260\nModel saved to models/PongNoFrameskip-v4_best_-0.26.pth\nTarget network updated at frame 287000\nNew best mean reward: -0.260 -> 0.110\nModel saved to models/PongNoFrameskip-v4_best_0.11.pth\nTarget network updated at frame 288000\nTarget network updated at frame 289000\nTarget network updated at frame 290000\nNew best mean reward: 0.110 -> 0.390\nModel saved to models/PongNoFrameskip-v4_best_0.39.pth\nTarget network updated at frame 291000\nTarget network updated at frame 292000\nNew best mean reward: 0.390 -> 0.770\nModel saved to models/PongNoFrameskip-v4_best_0.77.pth\nTarget network updated at frame 293000\nNew best mean reward: 0.770 -> 1.090\nModel saved to models/PongNoFrameskip-v4_best_1.09.pth\nTarget network updated at frame 294000\nTarget network updated at frame 295000\nNew best mean reward: 1.090 -> 1.430\nModel saved to models/PongNoFrameskip-v4_best_1.43.pth\nTarget network updated at frame 296000\nTarget network updated at frame 297000\nEpisode 160, Frame 297370, Reward: 18.0, Mean Reward (last 100): 1.760, Epsilon: 0.020, Speed: 72.38 f/s\nNew best mean reward: 1.430 -> 1.760\nModel saved to models/PongNoFrameskip-v4_best_1.76.pth\nTarget network updated at frame 298000\nTarget network updated at frame 299000\nNew best mean reward: 1.760 -> 2.120\nModel saved to models/PongNoFrameskip-v4_best_2.12.pth\nTarget network updated at frame 300000\nTarget network updated at frame 301000\nNew best mean reward: 2.120 -> 2.450\nModel saved to models/PongNoFrameskip-v4_best_2.45.pth\nTarget network updated at frame 302000\nNew best mean reward: 2.450 -> 2.850\nModel saved to models/PongNoFrameskip-v4_best_2.85.pth\nTarget network updated at frame 303000\nTarget network updated at frame 304000\nNew best mean reward: 2.850 -> 3.150\nModel saved to models/PongNoFrameskip-v4_best_3.15.pth\nTarget network updated at frame 305000\nTarget network updated at frame 306000\nNew best mean reward: 3.150 -> 3.530\nModel saved to models/PongNoFrameskip-v4_best_3.53.pth\nTarget network updated at frame 307000\nTarget network updated at frame 308000\nNew best mean reward: 3.530 -> 3.910\nModel saved to models/PongNoFrameskip-v4_best_3.91.pth\nTarget network updated at frame 309000\nTarget network updated at frame 310000\nNew best mean reward: 3.910 -> 4.200\nModel saved to models/PongNoFrameskip-v4_best_4.20.pth\nTarget network updated at frame 311000\nTarget network updated at frame 312000\nNew best mean reward: 4.200 -> 4.550\nModel saved to models/PongNoFrameskip-v4_best_4.55.pth\nTarget network updated at frame 313000\nNew best mean reward: 4.550 -> 4.930\nModel saved to models/PongNoFrameskip-v4_best_4.93.pth\nTarget network updated at frame 314000\nTarget network updated at frame 315000\nEpisode 170, Frame 315707, Reward: 17.0, Mean Reward (last 100): 5.230, Epsilon: 0.020, Speed: 72.75 f/s\nNew best mean reward: 4.930 -> 5.230\nModel saved to models/PongNoFrameskip-v4_best_5.23.pth\nTarget network updated at frame 316000\nTarget network updated at frame 317000\nNew best mean reward: 5.230 -> 5.590\nModel saved to models/PongNoFrameskip-v4_best_5.59.pth\nTarget network updated at frame 318000\nTarget network updated at frame 319000\nNew best mean reward: 5.590 -> 5.920\nModel saved to models/PongNoFrameskip-v4_best_5.92.pth\nTarget network updated at frame 320000\nTarget network updated at frame 321000\nNew best mean reward: 5.920 -> 6.210\nModel saved to models/PongNoFrameskip-v4_best_6.21.pth\nTarget network updated at frame 322000\nTarget network updated at frame 323000\nNew best mean reward: 6.210 -> 6.580\nModel saved to models/PongNoFrameskip-v4_best_6.58.pth\nTarget network updated at frame 324000\nTarget network updated at frame 325000\nNew best mean reward: 6.580 -> 6.890\nModel saved to models/PongNoFrameskip-v4_best_6.89.pth\nTarget network updated at frame 326000\nTarget network updated at frame 327000\nNew best mean reward: 6.890 -> 7.180\nModel saved to models/PongNoFrameskip-v4_best_7.18.pth\nTarget network updated at frame 328000\nTarget network updated at frame 329000\nNew best mean reward: 7.180 -> 7.500\nModel saved to models/PongNoFrameskip-v4_best_7.50.pth\nTarget network updated at frame 330000\nTarget network updated at frame 331000\nNew best mean reward: 7.500 -> 7.760\nModel saved to models/PongNoFrameskip-v4_best_7.76.pth\nTarget network updated at frame 332000\nTarget network updated at frame 333000\nNew best mean reward: 7.760 -> 8.070\nModel saved to models/PongNoFrameskip-v4_best_8.07.pth\nTarget network updated at frame 334000\nTarget network updated at frame 335000\nEpisode 180, Frame 335594, Reward: 17.0, Mean Reward (last 100): 8.430, Epsilon: 0.020, Speed: 73.23 f/s\nNew best mean reward: 8.070 -> 8.430\nModel saved to models/PongNoFrameskip-v4_best_8.43.pth\nTarget network updated at frame 336000\nTarget network updated at frame 337000\nNew best mean reward: 8.430 -> 8.650\nModel saved to models/PongNoFrameskip-v4_best_8.65.pth\nTarget network updated at frame 338000\nTarget network updated at frame 339000\nNew best mean reward: 8.650 -> 9.010\nModel saved to models/PongNoFrameskip-v4_best_9.01.pth\nTarget network updated at frame 340000\nTarget network updated at frame 341000\nNew best mean reward: 9.010 -> 9.310\nModel saved to models/PongNoFrameskip-v4_best_9.31.pth\nTarget network updated at frame 342000\nTarget network updated at frame 343000\nNew best mean reward: 9.310 -> 9.690\nModel saved to models/PongNoFrameskip-v4_best_9.69.pth\nTarget network updated at frame 344000\nTarget network updated at frame 345000\nNew best mean reward: 9.690 -> 10.030\nModel saved to models/PongNoFrameskip-v4_best_10.03.pth\nTarget network updated at frame 346000\nNew best mean reward: 10.030 -> 10.400\nModel saved to models/PongNoFrameskip-v4_best_10.40.pth\nTarget network updated at frame 347000\nTarget network updated at frame 348000\nNew best mean reward: 10.400 -> 10.720\nModel saved to models/PongNoFrameskip-v4_best_10.72.pth\nTarget network updated at frame 349000\nTarget network updated at frame 350000\nNew best mean reward: 10.720 -> 11.070\nModel saved to models/PongNoFrameskip-v4_best_11.07.pth\nTarget network updated at frame 351000\nTarget network updated at frame 352000\nNew best mean reward: 11.070 -> 11.340\nModel saved to models/PongNoFrameskip-v4_best_11.34.pth\nTarget network updated at frame 353000\nTarget network updated at frame 354000\nEpisode 190, Frame 354887, Reward: 15.0, Mean Reward (last 100): 11.620, Epsilon: 0.020, Speed: 81.20 f/s\nNew best mean reward: 11.340 -> 11.620\nModel saved to models/PongNoFrameskip-v4_best_11.62.pth\nTarget network updated at frame 355000\nTarget network updated at frame 356000\nNew best mean reward: 11.620 -> 11.960\nModel saved to models/PongNoFrameskip-v4_best_11.96.pth\nTarget network updated at frame 357000\nTarget network updated at frame 358000\nNew best mean reward: 11.960 -> 12.270\nModel saved to models/PongNoFrameskip-v4_best_12.27.pth\nTarget network updated at frame 359000\nTarget network updated at frame 360000\nNew best mean reward: 12.270 -> 12.560\nModel saved to models/PongNoFrameskip-v4_best_12.56.pth\nTarget network updated at frame 361000\nTarget network updated at frame 362000\nNew best mean reward: 12.560 -> 12.830\nModel saved to models/PongNoFrameskip-v4_best_12.83.pth\nTarget network updated at frame 363000\nTarget network updated at frame 364000\nNew best mean reward: 12.830 -> 13.070\nModel saved to models/PongNoFrameskip-v4_best_13.07.pth\nTarget network updated at frame 365000\nTarget network updated at frame 366000\nNew best mean reward: 13.070 -> 13.300\nModel saved to models/PongNoFrameskip-v4_best_13.30.pth\nTarget network updated at frame 367000\nTarget network updated at frame 368000\nNew best mean reward: 13.300 -> 13.620\nModel saved to models/PongNoFrameskip-v4_best_13.62.pth\nTarget network updated at frame 369000\nTarget network updated at frame 370000\nNew best mean reward: 13.620 -> 13.760\nModel saved to models/PongNoFrameskip-v4_best_13.76.pth\nTarget network updated at frame 371000\nTarget network updated at frame 372000\nNew best mean reward: 13.760 -> 14.090\nModel saved to models/PongNoFrameskip-v4_best_14.09.pth\nTarget network updated at frame 373000\nTarget network updated at frame 374000\nEpisode 200, Frame 374370, Reward: 19.0, Mean Reward (last 100): 14.360, Epsilon: 0.020, Speed: 72.99 f/s\nNew best mean reward: 14.090 -> 14.360\nModel saved to models/PongNoFrameskip-v4_best_14.36.pth\nTarget network updated at frame 375000\nTarget network updated at frame 376000\nNew best mean reward: 14.360 -> 14.620\nModel saved to models/PongNoFrameskip-v4_best_14.62.pth\nTarget network updated at frame 377000\nTarget network updated at frame 378000\nNew best mean reward: 14.620 -> 14.720\nModel saved to models/PongNoFrameskip-v4_best_14.72.pth\nTarget network updated at frame 379000\nTarget network updated at frame 380000\nNew best mean reward: 14.720 -> 14.850\nModel saved to models/PongNoFrameskip-v4_best_14.85.pth\nTarget network updated at frame 381000\nNew best mean reward: 14.850 -> 15.020\nModel saved to models/PongNoFrameskip-v4_best_15.02.pth\nTarget network updated at frame 382000\nTarget network updated at frame 383000\nNew best mean reward: 15.020 -> 15.180\nModel saved to models/PongNoFrameskip-v4_best_15.18.pth\nTarget network updated at frame 384000\nTarget network updated at frame 385000\nNew best mean reward: 15.180 -> 15.260\nModel saved to models/PongNoFrameskip-v4_best_15.26.pth\nTarget network updated at frame 386000\nTarget network updated at frame 387000\nNew best mean reward: 15.260 -> 15.390\nModel saved to models/PongNoFrameskip-v4_best_15.39.pth\nTarget network updated at frame 388000\nTarget network updated at frame 389000\nTarget network updated at frame 390000\nNew best mean reward: 15.390 -> 15.430\nModel saved to models/PongNoFrameskip-v4_best_15.43.pth\nTarget network updated at frame 391000\nNew best mean reward: 15.430 -> 15.560\nModel saved to models/PongNoFrameskip-v4_best_15.56.pth\nTarget network updated at frame 392000\nTarget network updated at frame 393000\nEpisode 210, Frame 393581, Reward: 20.0, Mean Reward (last 100): 15.670, Epsilon: 0.020, Speed: 79.56 f/s\nNew best mean reward: 15.560 -> 15.670\nModel saved to models/PongNoFrameskip-v4_best_15.67.pth\nTarget network updated at frame 394000\nTarget network updated at frame 395000\nNew best mean reward: 15.670 -> 15.760\nModel saved to models/PongNoFrameskip-v4_best_15.76.pth\nTarget network updated at frame 396000\nTarget network updated at frame 397000\nNew best mean reward: 15.760 -> 15.940\nModel saved to models/PongNoFrameskip-v4_best_15.94.pth\nTarget network updated at frame 398000\nTarget network updated at frame 399000\nNew best mean reward: 15.940 -> 16.060\nModel saved to models/PongNoFrameskip-v4_best_16.06.pth\nTarget network updated at frame 400000\nTarget network updated at frame 401000\nNew best mean reward: 16.060 -> 16.120\nModel saved to models/PongNoFrameskip-v4_best_16.12.pth\nTarget network updated at frame 402000\nNew best mean reward: 16.120 -> 16.200\nModel saved to models/PongNoFrameskip-v4_best_16.20.pth\nTarget network updated at frame 403000\nTarget network updated at frame 404000\nNew best mean reward: 16.200 -> 16.250\nModel saved to models/PongNoFrameskip-v4_best_16.25.pth\nTarget network updated at frame 405000\nTarget network updated at frame 406000\nNew best mean reward: 16.250 -> 16.340\nModel saved to models/PongNoFrameskip-v4_best_16.34.pth\nTarget network updated at frame 407000\nTarget network updated at frame 408000\nNew best mean reward: 16.340 -> 16.370\nModel saved to models/PongNoFrameskip-v4_best_16.37.pth\nTarget network updated at frame 409000\nTarget network updated at frame 410000\nNew best mean reward: 16.370 -> 16.420\nModel saved to models/PongNoFrameskip-v4_best_16.42.pth\nTarget network updated at frame 411000\nTarget network updated at frame 412000\nEpisode 220, Frame 412342, Reward: 18.0, Mean Reward (last 100): 16.480, Epsilon: 0.020, Speed: 72.41 f/s\nNew best mean reward: 16.420 -> 16.480\nModel saved to models/PongNoFrameskip-v4_best_16.48.pth\nTarget network updated at frame 413000\nTarget network updated at frame 414000\nNew best mean reward: 16.480 -> 16.540\nModel saved to models/PongNoFrameskip-v4_best_16.54.pth\nTarget network updated at frame 415000\nNew best mean reward: 16.540 -> 16.670\nModel saved to models/PongNoFrameskip-v4_best_16.67.pth\nTarget network updated at frame 416000\nTarget network updated at frame 417000\nTarget network updated at frame 418000\nNew best mean reward: 16.670 -> 16.700\nModel saved to models/PongNoFrameskip-v4_best_16.70.pth\nTarget network updated at frame 419000\nNew best mean reward: 16.700 -> 16.790\nModel saved to models/PongNoFrameskip-v4_best_16.79.pth\nTarget network updated at frame 420000\nTarget network updated at frame 421000\nNew best mean reward: 16.790 -> 16.800\nModel saved to models/PongNoFrameskip-v4_best_16.80.pth\nTarget network updated at frame 422000\nTarget network updated at frame 423000\nNew best mean reward: 16.800 -> 16.850\nModel saved to models/PongNoFrameskip-v4_best_16.85.pth\nTarget network updated at frame 424000\nTarget network updated at frame 425000\nNew best mean reward: 16.850 -> 16.880\nModel saved to models/PongNoFrameskip-v4_best_16.88.pth\nTarget network updated at frame 426000\nTarget network updated at frame 427000\nTarget network updated at frame 428000\nNew best mean reward: 16.880 -> 16.890\nModel saved to models/PongNoFrameskip-v4_best_16.89.pth\nTarget network updated at frame 429000\nTarget network updated at frame 430000\nEpisode 230, Frame 430400, Reward: 20.0, Mean Reward (last 100): 16.880, Epsilon: 0.020, Speed: 74.18 f/s\nTarget network updated at frame 431000\nTarget network updated at frame 432000\nNew best mean reward: 16.890 -> 16.910\nModel saved to models/PongNoFrameskip-v4_best_16.91.pth\nTarget network updated at frame 433000\nTarget network updated at frame 434000\nTarget network updated at frame 435000\nTarget network updated at frame 436000\nTarget network updated at frame 437000\nTarget network updated at frame 438000\nTarget network updated at frame 439000\nTarget network updated at frame 440000\nTarget network updated at frame 441000\nTarget network updated at frame 442000\nTarget network updated at frame 443000\nNew best mean reward: 16.910 -> 16.930\nModel saved to models/PongNoFrameskip-v4_best_16.93.pth\nTarget network updated at frame 444000\nTarget network updated at frame 445000\nNew best mean reward: 16.930 -> 16.980\nModel saved to models/PongNoFrameskip-v4_best_16.98.pth\nTarget network updated at frame 446000\nNew best mean reward: 16.980 -> 17.010\nModel saved to models/PongNoFrameskip-v4_best_17.01.pth\nTarget network updated at frame 447000\nTarget network updated at frame 448000\nEpisode 240, Frame 448628, Reward: 19.0, Mean Reward (last 100): 17.030, Epsilon: 0.020, Speed: 78.11 f/s\nNew best mean reward: 17.010 -> 17.030\nModel saved to models/PongNoFrameskip-v4_best_17.03.pth\nTarget network updated at frame 449000\nTarget network updated at frame 450000\nTarget network updated at frame 451000\nTarget network updated at frame 452000\nTarget network updated at frame 453000\nTarget network updated at frame 454000\nTarget network updated at frame 455000\nTarget network updated at frame 456000\nTarget network updated at frame 457000\nTarget network updated at frame 458000\nTarget network updated at frame 459000\nTarget network updated at frame 460000\nTarget network updated at frame 461000\nTarget network updated at frame 462000\nTarget network updated at frame 463000\nTarget network updated at frame 464000\nTarget network updated at frame 465000\nTarget network updated at frame 466000\nTarget network updated at frame 467000\nEpisode 250, Frame 467210, Reward: 19.0, Mean Reward (last 100): 17.030, Epsilon: 0.020, Speed: 73.02 f/s\nTarget network updated at frame 468000\nTarget network updated at frame 469000\nTarget network updated at frame 470000\nTarget network updated at frame 471000\nTarget network updated at frame 472000\nTarget network updated at frame 473000\nTarget network updated at frame 474000\nTarget network updated at frame 475000\nTarget network updated at frame 476000\nNew best mean reward: 17.030 -> 17.040\nModel saved to models/PongNoFrameskip-v4_best_17.04.pth\nTarget network updated at frame 477000\nTarget network updated at frame 478000\nNew best mean reward: 17.040 -> 17.110\nModel saved to models/PongNoFrameskip-v4_best_17.11.pth\nTarget network updated at frame 479000\nTarget network updated at frame 480000\nTarget network updated at frame 481000\nTarget network updated at frame 482000\nTarget network updated at frame 483000\nTarget network updated at frame 484000\nTarget network updated at frame 485000\nEpisode 260, Frame 485958, Reward: 21.0, Mean Reward (last 100): 17.080, Epsilon: 0.020, Speed: 71.55 f/s\nTarget network updated at frame 486000\nTarget network updated at frame 487000\nTarget network updated at frame 488000\nTarget network updated at frame 489000\nTarget network updated at frame 490000\nTarget network updated at frame 491000\nTarget network updated at frame 492000\nTarget network updated at frame 493000\nTarget network updated at frame 494000\nTarget network updated at frame 495000\nTarget network updated at frame 496000\nTarget network updated at frame 497000\nTarget network updated at frame 498000\nTarget network updated at frame 499000\nTarget network updated at frame 500000\nTarget network updated at frame 501000\nTarget network updated at frame 502000\nTarget network updated at frame 503000\nTarget network updated at frame 504000\nTarget network updated at frame 505000\nEpisode 270, Frame 505948, Reward: 12.0, Mean Reward (last 100): 16.890, Epsilon: 0.020, Speed: 77.51 f/s\nTarget network updated at frame 506000\nTarget network updated at frame 507000\nTarget network updated at frame 508000\nTarget network updated at frame 509000\nTarget network updated at frame 510000\nTarget network updated at frame 511000\nTarget network updated at frame 512000\nTarget network updated at frame 513000\nTarget network updated at frame 514000\nTarget network updated at frame 515000\nTarget network updated at frame 516000\nTarget network updated at frame 517000\nTarget network updated at frame 518000\nTarget network updated at frame 519000\nTarget network updated at frame 520000\nTarget network updated at frame 521000\nTarget network updated at frame 522000\nTarget network updated at frame 523000\nTarget network updated at frame 524000\nEpisode 280, Frame 524350, Reward: 20.0, Mean Reward (last 100): 17.070, Epsilon: 0.020, Speed: 72.00 f/s\nTarget network updated at frame 525000\nTarget network updated at frame 526000\nNew best mean reward: 17.110 -> 17.210\nModel saved to models/PongNoFrameskip-v4_best_17.21.pth\nTarget network updated at frame 527000\nTarget network updated at frame 528000\nTarget network updated at frame 529000\nTarget network updated at frame 530000\nTarget network updated at frame 531000\nTarget network updated at frame 532000\nTarget network updated at frame 533000\nTarget network updated at frame 534000\nTarget network updated at frame 535000\nTarget network updated at frame 536000\nTarget network updated at frame 537000\nTarget network updated at frame 538000\nTarget network updated at frame 539000\nTarget network updated at frame 540000\nTarget network updated at frame 541000\nTarget network updated at frame 542000\nEpisode 290, Frame 542503, Reward: 19.0, Mean Reward (last 100): 17.250, Epsilon: 0.020, Speed: 72.05 f/s\nNew best mean reward: 17.210 -> 17.250\nModel saved to models/PongNoFrameskip-v4_best_17.25.pth\nTarget network updated at frame 543000\nTarget network updated at frame 544000\nNew best mean reward: 17.250 -> 17.290\nModel saved to models/PongNoFrameskip-v4_best_17.29.pth\nTarget network updated at frame 545000\nTarget network updated at frame 546000\nTarget network updated at frame 547000\nNew best mean reward: 17.290 -> 17.310\nModel saved to models/PongNoFrameskip-v4_best_17.31.pth\nTarget network updated at frame 548000\nTarget network updated at frame 549000\nTarget network updated at frame 550000\nTarget network updated at frame 551000\nNew best mean reward: 17.310 -> 17.350\nModel saved to models/PongNoFrameskip-v4_best_17.35.pth\nTarget network updated at frame 552000\nNew best mean reward: 17.350 -> 17.430\nModel saved to models/PongNoFrameskip-v4_best_17.43.pth\nTarget network updated at frame 553000\nTarget network updated at frame 554000\nNew best mean reward: 17.430 -> 17.450\nModel saved to models/PongNoFrameskip-v4_best_17.45.pth\nTarget network updated at frame 555000\nTarget network updated at frame 556000\nNew best mean reward: 17.450 -> 17.590\nModel saved to models/PongNoFrameskip-v4_best_17.59.pth\nTarget network updated at frame 557000\nTarget network updated at frame 558000\nNew best mean reward: 17.590 -> 17.600\nModel saved to models/PongNoFrameskip-v4_best_17.60.pth\nTarget network updated at frame 559000\nEpisode 300, Frame 559759, Reward: 20.0, Mean Reward (last 100): 17.610, Epsilon: 0.020, Speed: 72.90 f/s\nNew best mean reward: 17.600 -> 17.610\nModel saved to models/PongNoFrameskip-v4_best_17.61.pth\nTarget network updated at frame 560000\nTarget network updated at frame 561000\nTarget network updated at frame 562000\nTarget network updated at frame 563000\nNew best mean reward: 17.610 -> 17.700\nModel saved to models/PongNoFrameskip-v4_best_17.70.pth\nTarget network updated at frame 564000\nTarget network updated at frame 565000\nNew best mean reward: 17.700 -> 17.720\nModel saved to models/PongNoFrameskip-v4_best_17.72.pth\nTarget network updated at frame 566000\nTarget network updated at frame 567000\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1570453852.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/522136768.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Early stopping if we're not learning after reasonable time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0mhas_complex\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"## Final Plot","metadata":{}},{"cell_type":"code","source":"# Plot final results\nif rewards:\n    plot_rewards(rewards, ma_window=100)\n    print(f\"Total episodes trained: {len(rewards)}\")\n    print(f\"Best reward achieved: {max(rewards) if rewards else 0}\")\n    print(f\"Mean reward of last 100 episodes: {np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T02:05:31.993142Z","iopub.execute_input":"2025-12-02T02:05:31.993413Z","iopub.status.idle":"2025-12-02T02:05:32.006047Z","shell.execute_reply.started":"2025-12-02T02:05:31.993393Z","shell.execute_reply":"2025-12-02T02:05:32.005189Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2867711575.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot final results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mplot_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total episodes trained: {len(rewards)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best reward achieved: {max(rewards) if rewards else 0}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'rewards' is not defined"],"ename":"NameError","evalue":"name 'rewards' is not defined","output_type":"error"}],"execution_count":11}]}